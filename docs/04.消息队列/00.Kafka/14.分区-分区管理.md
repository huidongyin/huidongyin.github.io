---
title: 分区-分区管理
date: 2023-01-01 00:00:00
tags:
  - Kafka
  - 消息队列
categories:
  - Kafka
description: 分区-分区管理
toc_number: false
author:
  name: huidong.yin
  link: https://huidongyin.github.io
permalink: /pages/14a08dc7-2466-3341-b742-a0ba9b8b26c3/
---

## 1.优先副本的选举

分区使用多副本机制来提升可靠性，但是只有leader副本对外提供读写服务，而follower副本只负责在内部进行消息同步。如果一个分区的leader副本不可用，那么就意味着整个分区不可用，此时就需要kafka从剩余的follower副本中挑选一个新的leader副本来继续对外提供服务。从某种角度讲，**broker节点中的leader副本个数的多少决定了这个节点负载的高低。**

在创建Topic的时候，该Topic的分区以及副本会尽可能均匀的分布到kafka集群的各个broker节点上，对应的leader副本的分配也比较均匀。

**针对同一个分区而言，同一个broker节点中不可能出现他的多个副本**。即kafka集群的一个broker中最多只能有它的一个副本，我们可以将leader副本所在的节点叫做分区的leader节点，而follower节点所在的broker节点可以叫做分区的follower节点。

如果kafka集群中的某一个broker挂掉了，也就是分区的leader节点下线，其中一个follower节点就会成为新的leader节点，这样会导致集群的负载不均衡，从而影响整体的健壮性和稳定性。当原来的leader节点恢复之后重新加入集群时，他只能成为一个新的follower节点而不再对外提供服务。

为了有效治理负载失衡的情况，kafka引入了优先副本（`preferred replica`）的概念。所谓的优先副本是指在AR集合中的第一个副本。理想情况下优先副本就是该分区的leader副本，所以也可以称为`preferred leader`。kafka要确保所有主题的优先副本在集群中均匀分布，这样就保证了所有分区的leader均匀分布。如果leader分布过于集中，就会造成集群负载不均衡。

所谓优先副本的选举是指通过一定的方式促使优先副本选举为leader副本，以此来促进集群的负载均衡，这一行为也叫做分区平衡。

> 分区平衡并不意味kafka集群的负载均衡，因为还要考虑集群中的分区分配是否均衡。另外每个分区leader副本的负载也是不同的；也就是说，就算集群中分区分配均衡，leader分配均衡，也不能保证整个集群的负载就是均衡的。


在kafka中可以提供分区自动平衡的功能（`rebalance`），与此对应的broker端参数是`auto.leader.rebalance.enable`，默认为true。如果开启分区自动平衡，kafka控制器会开启一个定时任务，轮询所有的broker节点，计算每个broker节点的分区不平衡率（broker的不平衡率=非优先副本的leader个数/分区总数）是否超过`leader.imbalance.per.broker.percentage`参数配置的比值，默认百分之十，如果超过这个值，会自动执行优先副本的选举动作以求分区平衡。执行周期由参数`leader.imbalance.check.interval.seconds`控制，默认300s。

生产环境不建议开启`rebalance`功能，可能引起负面的性能问题，也有可能引起客户端一定时间的阻塞。因为执行的时间无法自主掌控，如果在大促期间执行`rebalance`可能会造成业务阻塞，频繁超时的风险。分区和副本的均衡也不能确保集群整体的均衡，并且集群中一定程度上的不均衡是可以接受的，为防止关键时刻出问题，建议手动执行分区平衡。

kafka中的`kafka-perferred-replica-election.sh`脚本提供了对分区leader副本进行重新平衡的功能。优先副本的选举过程是一个安全的过程，kafka客户端可以自动感知分区leader副本的变更。

```bash
./kafka-preferred-replica-election.sh --zookeeper kafka-zookeeper:2181/kafka
```

> 在新版本的kafka中，这个脚本已经过期了，推荐使用`kafka-leader-election.sh`脚本。


---

## 2.分区重分配

当集群上的某个broker节点突然下线，如果节点上的分区是单副本的，那么这些分区就变的不可用了，相应的数据也就处于丢失状态；如果节点上的分区是多副本的，那么位于这个broker上的leader副本的角色会转交给集群的其他follower副本中。总之，这个节点上的分区副本都处于失效状态，kafka并不会将这些失效的分区副本自动的迁移到集群中剩余的可用broker上，如果放任不管，不仅会影响整个集群的负载均衡，还会影响整体服务的可用性和可靠性。

当要对集群中的一个节点进行有计划的下线操作的时候，为了保证分区副本的合理分配，我们希望通过某种方式能够将该节点上的分区副本迁移到其他的可用节点上。

当集群中新增broker时，只有新创建的主题分区才可能被分配到这个节点上，之前的主题分区并不会自动分配到新加入的节点，因为在他们被创建时还没有这个新节点，这样新节点的负载和原先节点之间的负载之间严重不平衡。

为了解决上述问题，需要让分区副本再次进行合理分配，也就是所谓的分区重分配。kafka提供了 `kafka-reassign-partition.sh`脚本来执行分区重分配的工作，他可以在集群扩容，broker节点失效的场景下对分区进行迁移。`kafka-reassign-partition.sh`脚本使用分为3个步骤：

1. 创建需要一个包含主题清单的JSON文件。
2. 根据主题清单和broker节点清单生成一份重分配方案。
3. 根据这份方案执行具体的分配动作。

下面通过一个具体的案例来演示 `kafka-reassign-partition.sh` 脚本的用法。

1. 首先在一个由三个broker节点组成的集群中创建一个Topic `topic-reassign`，主题中包含4个分区和两个副本。

```bash
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin# ./kafka-topics.sh --zookeeper kafka-zookeeper:2181/kafka --create --topic topic-reassign --replication-factor 2 --partitions 4

root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin# ./kafka-topics.sh --zookeeper kafka-zookeeper:2181/kafka --describe --topic topic-reassign
Topic: topic-reassign	TopicId: K1UON9X_QbaqfVbOV4skdw	PartitionCount: 4	ReplicationFactor: 2	Configs: 
	Topic: topic-reassign	Partition: 0	Leader: 0	Replicas: 0,1	Isr: 0,1
	Topic: topic-reassign	Partition: 1	Leader: 1	Replicas: 1,2	Isr: 1,2
	Topic: topic-reassign	Partition: 2	Leader: 2	Replicas: 2,0	Isr: 2,0
	Topic: topic-reassign	Partition: 3	Leader: 0	Replicas: 0,2	Isr: 0,2
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin#
```

可以观察到Topic`topic-reassign`在3个节点上都有对应的分区副本分布。由于某种原因，我们想要下线其中一台broker，在此之前，我们需要将上面的分区副本迁移出去。使用`kafka-reassign-partition.sh`脚本的第一步就是要创建一个JSON文件，文件内容为要进行分区重新分配的主题清单。

```json
{
  "topics": [
    {
      "topic": "topic-reassign"
    }
  ],
  "version": 1
}
```

第二步就是根据这个JSON文件和指定所要分配的broker节点列表来生成一份候选的重分配方案。

```bash
./kafka-reassign-partitions.sh --zookeeper kafka-zookeeper:2181/kafka --generate --topics-to-move-json-file reassign.json --broker-list 0,2

Current partition replica assignment
{"version":1,"partitions":[{"topic":"topic-reassign","partition":0,"replicas":[0,1],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":1,"replicas":[1,2],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":2,"replicas":[2,0],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":3,"replicas":[0,2],"log_dirs":["any","any"]}]}

Proposed partition reassignment configuration
{"version":1,"partitions":[{"topic":"topic-reassign","partition":0,"replicas":[2,0],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":1,"replicas":[0,2],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":2,"replicas":[2,0],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":3,"replicas":[0,2],"log_dirs":["any","any"]}]}
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin#
```

> `broker-list` 参数指定所要分配的broker节点列表。


上面示例打印出两个json的内容，第一个表示当前分区副本分配情况，第二个表示重分配的新方案。我们将新方案保存到一个名为`project.json`的文件。

第三步执行具体的重分配动作：

```bash
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin#./kafka-reassign-partitions.sh --zookeeper kafka-zookeeper:2181/kafka --execute --reassignment-json-file project.json

Current partition replica assignment

{"version":1,"partitions":[{"topic":"topic-reassign","partition":0,"replicas":[0,1],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":1,"replicas":[1,2],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":2,"replicas":[2,0],"log_dirs":["any","any"]},{"topic":"topic-reassign","partition":3,"replicas":[0,2],"log_dirs":["any","any"]}]}

Save this to use as the --reassignment-json-file option during rollback
Successfully started partition reassignments for topic-reassign-0,topic-reassign-1,topic-reassign-2,topic-reassign-3
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin#
```

再次查看Topic`topic-reassign`的具体信息：

```
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin# ./kafka-topics.sh --zookeeper kafka-zookeeper:2181/kafka --describe --topic topic-reassign
Topic: topic-reassign	TopicId: K1UON9X_QbaqfVbOV4skdw	PartitionCount: 4	ReplicationFactor: 2	Configs: 
	Topic: topic-reassign	Partition: 0	Leader: 2	Replicas: 2,0	Isr: 0,2
	Topic: topic-reassign	Partition: 1	Leader: 0	Replicas: 0,2	Isr: 2,0
	Topic: topic-reassign	Partition: 2	Leader: 2	Replicas: 2,0	Isr: 2,0
	Topic: topic-reassign	Partition: 3	Leader: 0	Replicas: 0,2	Isr: 0,2
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin#
```

可以看到主题中的所有分区副本都只在0和2的broker节点上分布了。

除了让脚本自动生成候选方案，用户还可以自定义重分配方案，这样就不需要指定第一步和第二步了。

分区重分配的基本原理是先通过控制器为每个分区添加新副本（增加副本因子），新的副本将从分区的leader副本那里复制所有的数据。根据分区的大小不同，复制过程可能要花一些时间，因为数据是通过网络复制到新的副本上的。在复制完成之后，控制器将旧的副本从副本清单中移除（恢复为原来的副本因子）。注意重分配过程中要确保有足够的空间。

对于分区重分配而言，还有可选的第四步：验证查看分区重分配的进度，只需要将上面的execute替换成verify即可。

```bash
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin# ./kafka-reassign-partitions.sh --zookeeper kafka-zookeeper:2181/kafka --verify --reassignment-json-file project.json

Status of partition reassignment:
Reassignment of partition topic-reassign-0 is complete.
Reassignment of partition topic-reassign-1 is complete.
Reassignment of partition topic-reassign-2 is complete.
Reassignment of partition topic-reassign-3 is complete.
Clearing broker-level throttles on brokers 0,1,2
Clearing topic-level throttles on topic topic-reassign
```

分区重分配对集群性能有很大影响，需要占用额外的资源，比如网络和磁盘。在实际操作中，我们将降低重分配的粒度，分成多个小批次来执行，以此来将负面影响降到最低。

**如果要将某个broker下线，那么在执行分区重分配动作之前最好先关闭或者重启broker，这样这个broker就不在是任何分区的leader节点了，他的分区就可以被分配给集群中的其他broker。这样可以减少broker之间的流量复制，以此提升重分配的性能，以及减少对集群的影响。**

---

## 3.复制限流

分区重分配的本质在于数据复制，先增加新的副本，然后进行数据同步，最后删除旧的副本来达到最终目的。数据复制会占用额外的资源，如果重分配的量太大必然会严重影响整体性能，尤其是处于业务高峰期的时候。减小重分配的粒度，以小批次的方式来操作是一种可行的解决思路。如果集群中某个主题或者某个分区的流量在一段时间特别大，那么只靠减小粒度是不足以应对的，这时就需要有一个限流的机制，可以对副本间的复制流量加以限制来保证重分配期间整体服务不会受到太大的影响。

副本间的复制限流有两种实现方式，`kafka-configs.sh`脚本和`kafka-reassign-partitions.sh`脚本。

---

### 3.1`kafka-configs.sh`脚本实现复制限流

`kafka-configs.sh`脚本主要以动态配置的方式来达到限流的目的，在broker级别有两个与复制限流相关的配置参数：`follower.replication.throttled.rate`和`leader.replication.throttled.rate`，前者用于设置follower副本复制的速度，后者用于设置leader副本传输的速度，单位是B/s。通常两者的配置值相同。下面的示例中将某个broker中的leader副本和follower副本的复制速度限制在1024B/s之内，即1KB/s。

```bash
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin# ./kafka-configs.sh --zookeeper kafka-zookeeper:2181/kafka --entity-type brokers --entity-name 1 --alter --add-config follower.replication.throttled.rate=1024,leader.replication.throttled.rate=1024
Completed updating config for entity: brokers '1'.
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin#
```

我们再来查看一下broker中刚刚添加的配置：

```bash
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin# ./kafka-configs.sh --zookeeper kafka-zookeeper:2181/kafka --entity-type brokers --entity-name 1 --describe
Configs for brokers '1' are leader.replication.throttled.rate=1024,follower.replication.throttled.rate=1024
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin#
```

删除刚刚添加的配置也很简单：

```bash
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin# ./kafka-configs.sh --zookeeper kafka-zookeeper:2181/kafka --entity-type brokers --entity-name 1 --alter --delete-config follower.replication.throttled.rate,leader.replication.throttled.rate

Completed updating config for entity: brokers '1'.
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin#
```

在主题级别也有两个相关参数来限制复制的速度：`leader.replication.throttled.replicas`和 `follower.replication.throttled.replicas`，他们分别用来配置被限制速度的主题所对应的leader副本列表和follower副本列表。为了演示用法，先来创建一个分区数为3副本数为2的主题topic-throttle，并查看他的详细信息。

```bash
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin# ./kafka-topics.sh --zookeeper kafka-zookeeper:2181/kafka --create --topic topic-throttle --replication-factor 2 --partitions 3
Created topic topic-throttle.
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin# ./kafka-topics.sh --zookeeper kafka-zookeeper:2181/kafka --describe --topic topic-throttle
Topic: topic-throttle	TopicId: 2LpemHVoTVe6BbgSFNDjFA	PartitionCount: 3	ReplicationFactor: 2	Configs: 
	Topic: topic-throttle	Partition: 0	Leader: 2	Replicas: 2,1	Isr: 2,1
	Topic: topic-throttle	Partition: 1	Leader: 0	Replicas: 0,2	Isr: 0,2
	Topic: topic-throttle	Partition: 2	Leader: 1	Replicas: 1,0	Isr: 1,0
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin#
```

主题`topic-throttle`的三个分区所对应的leader节点分别为0，1，,2，即分区与代理的映射关系为`0:2,1:0,2:1`，而对应的follower节点分别为1,2,0，相关的分区与代理的映射关系为`0:1,1:2,2:0`，那么此主题的限流副本列表及具体的操作细节如下：

```bash
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin# ./kafka-configs.sh --zookeeper kafka-zookeeper:2181/kafka --entity-type topics --entity-name topic-throttle --alter --add-config leader.replication.throttled.replicas=[0:0,1:1,2:2],follower.replication.throttled.replicas=[0:1,1:2,2:0]

Completed updating config for entity: topic 'topic-throttle'.
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin#
```

在了解了与限流相关的4个配置参数后，我们演示一下带有限流的分区重分配的用法。首先按照上一节的玩法创建一个包含可行性方案的`project.json`文件，内容如下：

```json
{
  "version": 1,
  "partitions": [
    {
      "topic": "topic-throttle",
      "partition": 1,
      "replicas": [
        2,
        0
      ],
      "log_dirs": [
        "any",
        "any"
      ]
    },
    {
      "topic": "topic-throttle",
      "partition": 0,
      "replicas": [
        0,
        2
      ],
      "log_dirs": [
        "any",
        "any"
      ]
    },
    {
      "topic": "topic-throttle",
      "partition": 2,
      "replicas": [
        0,
        2
      ],
      "log_dirs": [
        "any",
        "any"
      ]
    }
  ]
}
```

接下来设置被限流的副本列表，这里要注意，首先看一下重分配前和分配后的分区副本对比，详细如下：

```
partition   重分配前的AR   分配后的预期AR
0             0，1         0，2 
1             1，2         2，0
2             2，0         0，2
```

如果分区重新分配会引起某个分区AR集合变更，那么这个分区中的leader有关的限制会应用于重分配前的所有副本，因为任何一个副本都肯呢个是leader，而与follower有关的限制会应用于所有移动的目的地。举个例子：对于上面的布局对比而言，分区0重分配的AR为[0,1]，重分配后的AR为[0,2]，那么这里的目的地就是新增的2.也就是说，对分区0而言，`leader.replication.throttled.replicas`配置为[0:0,0:1]，`follower.replication.throttled.replicas`配置为[0:2]。同理对于分区1而言，`leader.replication.throttled.replicas`配置为[1:1，1:2]，`follower.replication.throttled.replicas`配置为[1:0]。分区3的AR集合没有任何变化，这里可以忽略。

获取限流副本列表之后，我们就可以执行具体的操作了：

```bash
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin# ./kafka-configs.sh --zookeeper kafka-zookeeper:2181/kafka --entity-type topics --entity-name topic-throttle --alter --add-config leader.replication.throttled.replicas=[1:1,1:2,0:0,0:1],follower.replication.throttled.replicas=[1:0,0:2]

Completed updating config for entity: topic 'topic-throttle'.
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin#
```

接下来在设置broker2的复制速度为10B/s，这样在下面的操作中可以很方便的观察限流与不限流的不同：

```bash
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin# ./kafka-configs.sh --zookeeper kafka-zookeeper:2181/kafka --entity-type brokers --entity-name 2 --alter --add-config leader.replication.throttled.rate=10,follower.replication.throttled.rate=10

Completed updating config for entity: brokers '2'.
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin#
```

在执行具体的重分配操作之前，我们需要开启一个生产者向主题`topic-throttle`中发送一批消息，这样可以方便观察正在进行数据复制的过程。

之后我们在执行正常的分区重新分配的操作：

```bash
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin# ./kafka-reassign-partitions.sh --zookeeper kafka-zookeeper:2181/kafka --execute --reassignment-json-file project.json

Current partition replica assignment

{"version":1,"partitions":[{"topic":"topic-throttle","partition":0,"replicas":[2,1],"log_dirs":["any","any"]},{"topic":"topic-throttle","partition":1,"replicas":[0,2],"log_dirs":["any","any"]},{"topic":"topic-throttle","partition":2,"replicas":[1,0],"log_dirs":["any","any"]}]}

Save this to use as the --reassignment-json-file option during rollback
Successfully started partition reassignments for topic-throttle-0,topic-throttle-1,topic-throttle-2
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin#
```

执行之后可以查看执行的进度：

```bash
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin# ./kafka-reassign-partitions.sh --zookeeper kafka-zookeeper:2181/kafka --verify --reassignment-json-file project.json

Status of partition reassignment:
Reassignment of partition topic-throttle-0 is complete.
Reassignment of partition topic-throttle-1 is complete.
Reassignment of partition topic-throttle-2 is complete.
Clearing broker-level throttles on brokers 0,1,2
Clearing topic-level throttles on topic topic-throttle
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin#
```

注意到最后两行，提示之前针对broker和topic级别做的两个限流操作已经移除了。

---

### 3.2`kafka-reassign-partitions.sh`脚本实现限流

`kafka-reassign-partitions.sh`脚本也提供了限流功能，只需要一个参数`throttle`参数即可。

```bash
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin# ./kafka-reassign-partitions.sh --zookeeper kafka-zookeeper:2181/kafka --execute --reassignment-json-file project.json --throttle 10

Current partition replica assignment

{"version":1,"partitions":[{"topic":"topic-throttle","partition":0,"replicas":[0,2],"log_dirs":["any","any"]},{"topic":"topic-throttle","partition":1,"replicas":[2,0],"log_dirs":["any","any"]},{"topic":"topic-throttle","partition":2,"replicas":[0,2],"log_dirs":["any","any"]}]}

Save this to use as the --reassignment-json-file option during rollback
Warning: You must run --verify periodically, until the reassignment completes, to ensure the throttle is removed.
The inter-broker throttle limit was set to 10 B/s
Successfully started partition reassignments for topic-throttle-0,topic-throttle-1,topic-throttle-2
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin#
```

上面的信息包含了明确的警告信息：需要周期行的执行查看进度的命令直到重分配完成，这样可以确保限流设置被移除。也就是说使用这种方式的限流同样需要显式的执行某些操作以便在重分配完成之后可以删除限流的设置。上面的信息还告知了目前限流的速度上限为10B/s。

如果想在重分配期间修改限制来增加吞吐量，以便完成的更快，则可以重新运行`kafka-reassign-partitions.sh`脚本的`execute`命令，使用相同的`reassignment-json-file`。

```bash
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin# ./kafka-reassign-partitions.sh --zookeeper kafka-zookeeper:2181/kafka --execute --reassignment-json-file project.json --throttle 1024

Current partition replica assignment

{"version":1,"partitions":[{"topic":"topic-throttle","partition":0,"replicas":[0,2],"log_dirs":["any","any"]},{"topic":"topic-throttle","partition":1,"replicas":[2,0],"log_dirs":["any","any"]},{"topic":"topic-throttle","partition":2,"replicas":[0,2],"log_dirs":["any","any"]}]}

Save this to use as the --reassignment-json-file option during rollback
Warning: You must run --verify periodically, until the reassignment completes, to ensure the throttle is removed.
The inter-broker throttle limit was set to 1024 B/s
Successfully started partition reassignments for topic-throttle-0,topic-throttle-1,topic-throttle-2
root@b531dc9d377f:/opt/kafka_2.13-2.8.1/bin#
```

这样限流速度上限为1024。其实`kafka-reassign-partitions.sh`脚本的实现原理就是配置与限流相关的4个参数，相比于`kafka-configs.sh`脚本的方式更加简单不容易出错。

---

## 4.修改副本因子

在创建主题之后我们还可以修改分区的个数，同样可以修改副本因子(副本数)。修改副本因子的使用场景也很多，比如在创建主题时填写了错误的副本因子数而需要修改，再比如运行段时间之后想要通过增加副本因子数来提高容错性和可靠性。

本节中修改副本因子的功能也是通过重分配所使用的 `kafka-reassign-partition.sh`脚本实现的。回过头来看上一节用到的`project.json`文件。

```json
{
  "version": 1,
  "partitions": [
    {
      "topic": "topic-throttle",
      "partition": 1,
      "replicas": [
        2,
        0
      ],
      "log_dirs": [
        "any",
        "any"
      ]
    },
    {
      "topic": "topic-throttle",
      "partition": 0,
      "replicas": [
        0,
        2
      ],
      "log_dirs": [
        "any",
        "any"
      ]
    },
    {
      "topic": "topic-throttle",
      "partition": 2,
      "replicas": [
        0,
        2
      ],
      "log_dirs": [
        "any",
        "any"
      ]
    }
  ]
}
```

可以观察到JSON 内容里的 replicas 都是 2 个副本，我们可以自行添加一个副本，比如对分区 1 而言，可以改成下面的内容:

```json
    {
  "topic": "topic-throttle",
  "partition": 1,
  "replicas": [
    2,
    1,
    0
  ],
  "log_dirs": [
    "any",
    "any",
    "any"
  ]
}
```

我们可以将其他分区的 replicas 内容也改成[0,1,2]，这样每个分区的副本因子就都从 2增加到了3。注意增加副本因子时也要在 `log_dirs` 中添加一个`any`，这个 `log_dirs` 代表kafka中的日志日录，对应于 broker 端的 `log.dir`或 `log.dirs` 参数的配置值，如果不需要关注此方面的细节，那么可以简单地设置为 `any`。我们将修改后的JSON内容保存为新`add.json` 文件。在执行 `kafka-reassign-partition.sh` 脚本前，主题 `topic-throttle`的详细信息(副本因子为2) 如下:

```bash
root@101ed4754423:/opt/kafka_2.13-2.8.1/bin# ./kafka-topics.sh --zookeeper kafka-zookeeper:2181 --describe --topic topic-throttle
Topic: topic-throttle	TopicId: 7HSKjZkrRhiiGTGl7mZeVA	PartitionCount: 3	ReplicationFactor: 2	Configs: 
	Topic: topic-throttle	Partition: 0	Leader: 1	Replicas: 1,2	Isr: 1,2
	Topic: topic-throttle	Partition: 1	Leader: 2	Replicas: 2,0	Isr: 2,0
	Topic: topic-throttle	Partition: 2	Leader: 0	Replicas: 0,1	Isr: 0,1
root@101ed4754423:/opt/kafka_2.13-2.8.1/bin# 
```

执行`kafka-reassign-partition.sh` 脚本的`execute`。

```bash
root@101ed4754423:/opt/kafka_2.13-2.8.1/bin# ./kafka-reassign-partitions.sh --zookeeper kafka-zookeeper:2181 --execute --reassignment-json-file add.json

Current partition replica assignment

{"version":1,"partitions":[{"topic":"topic-throttle","partition":0,"replicas":[1,2],"log_dirs":["any","any"]},{"topic":"topic-throttle","partition":1,"replicas":[2,0],"log_dirs":["any","any"]},{"topic":"topic-throttle","partition":2,"replicas":[0,1],"log_dirs":["any","any"]}]}

Save this to use as the --reassignment-json-file option during rollback
Successfully started partition reassignments for topic-throttle-0,topic-throttle-1,topic-throttle-2
root@101ed4754423:/opt/kafka_2.13-2.8.1/bin# 
```

执行之后再次查看主题 `topic-throttle` 的详细信息。

```bash
root@101ed4754423:/opt/kafka_2.13-2.8.1/bin# ./kafka-topics.sh --zookeeper kafka-zookeeper:2181 --describe --topic topic-throttle
Topic: topic-throttle	TopicId: 7HSKjZkrRhiiGTGl7mZeVA	PartitionCount: 3	ReplicationFactor: 3	Configs: 
	Topic: topic-throttle	Partition: 0	Leader: 1	Replicas: 0,1,2	Isr: 1,2,0
	Topic: topic-throttle	Partition: 1	Leader: 2	Replicas: 2,1,0	Isr: 2,0,1
	Topic: topic-throttle	Partition: 2	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2
```

可以看到相应的副本因子数已经增加到3了。

与修改分区数不同的是，副本数还可以减小，这里我们通过`kafka-reassign-partition.sh`脚本来减少分区的副本因子。再次修改`project.json`文件中的内容，内容参考如下：

```json
{
  "version": 1,
  "partitions": [
    {
      "topic": "topic-throttle",
      "partition": 1,
      "replicas": [
        2
      ],
      "log_dirs": [
        "any"
      ]
    },
    {
      "topic": "topic-throttle",
      "partition": 0,
      "replicas": [
        1
      ],
      "log_dirs": [
        "any"
      ]
    },
    {
      "topic": "topic-throttle",
      "partition": 2,
      "replicas": [
        0
      ],
      "log_dirs": [
        "any"
      ]
    }
  ]
}
```

再次执行`kafka-reassign-partition.sh` 脚本的`execute`。

```bash
root@101ed4754423:/opt/kafka_2.13-2.8.1/bin# ./kafka-reassign-partitions.sh --zookeeper kafka-zookeeper:2181 --execute --reassignment-json-file add.json

Current partition replica assignment

{"version":1,"partitions":[{"topic":"topic-throttle","partition":0,"replicas":[0,1,2],"log_dirs":["any","any","any"]},{"topic":"topic-throttle","partition":1,"replicas":[2,1,0],"log_dirs":["any","any","any"]},{"topic":"topic-throttle","partition":2,"replicas":[0,1,2],"log_dirs":["any","any","any"]}]}

Save this to use as the --reassignment-json-file option during rollback
Successfully started partition reassignments for topic-throttle-0,topic-throttle-1,topic-throttle-2
root@101ed4754423:/opt/kafka_2.13-2.8.1/bin# 
```

主题`topic-throttle`的详细信息如下：

```bash
root@101ed4754423:/opt/kafka_2.13-2.8.1/bin# ./kafka-topics.sh --zookeeper kafka-zookeeper:2181 --describe --topic topic-throttle
Topic: topic-throttle	TopicId: 7HSKjZkrRhiiGTGl7mZeVA	PartitionCount: 3	ReplicationFactor: 1	Configs: 
	Topic: topic-throttle	Partition: 0	Leader: 1	Replicas: 1	Isr: 1
	Topic: topic-throttle	Partition: 1	Leader: 2	Replicas: 2	Isr: 2
	Topic: topic-throttle	Partition: 2	Leader: 0	Replicas: 0	Isr: 0
```

可以看到主题 `topic-throttle` 的副本因子又被修改为 1 了。我们执行`kafka-reassign-partition.sh` 脚本(execute)所使用的候选方案都是手动修改的，在增加副本因子的时候由于整个示例集群中只有 3个 broker 节点，从 2 增加到 3只需填满副本即可。再者,示例中减少副本因子的时候改成了 1，这样可以简单地把各个 broker节点轮询一遍，如此也就不太会有负载不均衡的影响。不过在真实应用中，可能面对的是一个包含了几十个 broker 节点的集群，将副本数从2 修改为 5，或者从 4 修改为 3 的时候，如何进行合理的分配是一个关键的问题。我们可以通过程序来计算出分配方案，假设已经确定了主题的分区数、副本因子数量和可用的Broker 数。

```java
public class KafkaReplicaAssignment {

    public static void main(String[] args) {
        int partitions = 10; // 主题的分区数
        int replicationFactor = 2; // 副本因子数量
        int brokers = 4; // 可用的 Broker 数

        List<List<Integer>> replicaAssignment = replicaAssignment(brokers, replicationFactor, partitions);

        // 打印分区的副本分配
        for (int partition = 0; partition < partitions; partition++) {
            List<Integer> replicas = replicaAssignment.get(partition);
            System.out.println("Partition " + partition + " replicas: " + replicas);
        }
    }

    public static List<List<Integer>> replicaAssignment(int brokers, int factor, int partitions) {
        List<List<Integer>> replicaAssignment = new ArrayList<>();
        for (int partition = 0; partition < partitions; partition++) {
            List<Integer> replicas = new ArrayList<>();
            for (int i = 0; i < factor; i++) {
                replicas.add((partition + i) % brokers);
            }
            replicaAssignment.add(replicas);
        }
        return replicaAssignment;
    }
}
```

----



