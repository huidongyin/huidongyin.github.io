---
title: 日志存储-日志格式的演变
date: 2023-11-19 09:40:39
permalink: /pages/600b7ed6-d718-3e2d-8577-335538ff687a/
categories:
  - 消息队列
  - Kafka
tags:
  - 
author: 
  name: huidong.yin
  link: https://huidongyin.github.io
---

随着Kafka的发展，其消息格式也在不断升级改造，从`0.8.x`版本开始到`2.0.0`版本，Kafka的消息格式也经历了三个版本：`v0版本`，`v1版本`，`v2版本`。

每个分区由内部的每一条消息组成，如果消息格式设计得不够精炼，那么其功能和性能者会大打折扣。比如有冗余字段，势必会不必要地增加分区的占用空间，进而不仅使存储的开销变大、网络传输的开销变大，也会使 Kafka 的性能下降。反观如果缺少字段，比如在最初的 Kafka 消息版本中没有 timestamp 字段，对内部而言，其影响了日志保存、切分策略，对外部而言其影响了消息审计、端到端延迟、大数据应用等功能的扩展。虽然可以在消息体内部添加一个时间戳，但解析变长的消息体会带来额外的开销，而存储在消息体(参考下图中的 value字段)前面可以通过指针偏移量获取其值而容易解析，进而减少了开销(可以查看 v1 版本)，虽然相比于没有 timestamp 字段的开销会大一点。由此可见，仅在一个字段的一增一减之间有这么多门道，那么 Kafka 具体是怎么做的呢? 本节只针对 Kafka `0.8.x` 之上(包含)的版本做相应说明，对于之前的版本不做陈述。

---

## 1.V0版本

Kafka 消息格式的第一个版本通常称为v0版本，在 Kafka `0.10.0` 之前都采用的这个消息格式，在 `0.8.x` 版本之前，Kafka 还使用过一个更古老的消息格式，不过对目前的 Kafka 而言，我们也不需要了解这个版本的消息格式。如无特殊说明，只讨论消息未压缩的情形。

下图中左边的`RECORD`部分就是 v0 版本的消息格式，大多数人会把图中左边的整体(即包括 `offset` 和 `message size` 字段)都看作消息，因为每个 `RECORD` (v0 和 vl版)必定对应一个 `offset` 和 `message size`。每条消息都有一个 `offset` 用来标志它在分区中的偏移量，这个 `offset` 是逻辑值，而非实际物理偏移值，`message size` 表示消息的大小，这两者在一起被称为日志头部(`LOG OVERHEAD`)，固定为 12B。`LOG OVERHEAD` 和`RECORD` 一起用来描述一条消息，为了配合陈述的语境，在讲述具体消息格式时会偏向于将单纯的 `RECORD` 看作消息，而在其他地方则偏向于将 `LOG OVERHEAD` 和 `RECORD` 的整体看作消息。与消息对应的还有消息集的概念，消息集中包含一条或多条消息，消息集不仅是存储于磁盘及在网络上传输(`Produce & Fetch`)的基本形式，而且是 Kafka中压缩的基本单元，详细结构参考下图中的右边部分。

![](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311190950613.png)

v0 版本中一个消息的最小长度(`RECORD_OVERHEAD_V0`)为 `crc32 + magic + attributes + key length + value length = 4B + 1B + 1B + 4B + 4B = 14B`。也就是说，v0 版本中一条消息的最小长度为 14B，如果小于这个值，那么这就是一条破损的消息而不被接收。

---

## 2.V1版本

Kafka从`0.10.0`版本开始到0.11.0版本之前所使用的消息格式版本为V1，比V0版本就多了一个timestamp字段，表示消息的时间戳。V1版本的消息结构如下：

![](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311190955637.png)

v1 版本的 magic 字段的值为1。v1 版本的 attributes 字段中的低3位和v0版本的一样,还是表示压缩类型,而第4个位(bit)也被利用了起来:0表示timestamp类型为`CreateTime`,而1表示 timestamp 类型为 `LogAppendTime`，其他位保留。timestamp 类型由 broker 端参数`log.message.timestamp.type` 来配置，默认值为 `CreateTime`，即采用生产者创建消息时的时间戳。如果在创建 `ProducerRecord` 时没有显式指定消息的时间戳，那么 `KafkaProducer` 也会在发送这条消息前自动添加上。下面是 `KafkaProducer` 中与此对应的一句关键代码:

```text
long timestamp = record.timestamp() == null ? time.milliseconds () : record.timestamp () ;
```

> v1 版本的消息的最小长度 (`RECORD_OVERHEAD_V1`)要比 v0 版本的大 8 个字节，即22B。

---

## 2.3 消息压缩

常见的压缩算法是数据量越大压缩效果越好，一条消息通常不会太大，这就导致压缩效果不是太好，而 Kafka 实现的压缩方式是将多条消息一起进行压缩，这样可以保证较好的压缩效果，通常，生产者发送的压缩数据在 broker 中也是保持压缩状态进行存储的，消费者从服务端获取的也是压缩的消息，消费者在处理消息之前才会解压消息，这样保持了端到端的压缩。

Kafka 日志中使用哪种压缩方式是通过参数 `compression.type` 来配置的，默认值`producer`，表示保留生产者使用的压缩方式。这个参数还可以配置为`gzip,snapy,lz4`分别对应 GZIP、SNAPPY、LZ4 这3种压缩算法。如果参数 `compression.type` 配置为`uncompressed`，则表示不压缩。

> 压缩率是压缩后的大小与压缩前的对比。例如:把 100MB 的文件压缩后是9OMB压缩率为 `90/100x100%=90%`，压缩率越小，压缩效果越好。

以上都是针对消息未压缩的情况，而当消息压缩时是将整个消息集进行压缩作为内层消(inner message)，内层消息整体作为外层 (wrapper message)的 value，其结构如下图所示。

压缩后的外层消息 (wrapper message)中的 key 为 null，所以下图左半部分没有画出key字段，value 字段中保存的是多条压缩消息 (inner message，内层消息)，其中 Record表示是从 crc32 到 value 的消息格式。当生产者创建压缩消息的时候，对内部压缩消息设置的offset 从0 开始为每个内部消息分配 offset。

![](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311191005430.png)
![](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311191005911.png)

其实每个从生产者发出的消息集中的消息 offset 都是从0开始的，当然这个 offset 不能直接存储在日志文件中，对 offset 的转换是在服务端进行的，客户端不需要做这个工作。外层消息保存了内层消息中最后一条消息的绝对位移 (absolute offset)，绝对位移是相对于整个分区而言的。参考上图，对于未压缩的情形，图右内层消息中最后一条的 offset 理应是 1030，但被压缩之后就变成了 5，而这个 1030 被赋予给了外层的 offset。当消费者消费这个消息集的时候,首先解压缩整个消息集，然后找到内层消息中最后一条消息的 inner offset，根据如下公式找到内层消息中最后一条消息前面的消息的 absolute offset (RO 表示 Relative Offset，IO 表示 InnerOffset，而 AO 表示 Absolute Offset):

```text
RO = IO_of a message - IO_of_the_last message
AO = AO_Of Last Inner_Message + RO
```

这里的 RO 是前面的消息相对最后一条消息的 IO 而言的，所以其值小于等于 0，0 表示最后一条消息自身。

> 压缩消息，英文是 `compress message`，Kafka 中还有一个 `compact message`，常常被直译成压缩消息，需要注意两者的区别。`compact message` 是针对日志清理策略而言的(`cleanup.policy= compact`)，是指日志压缩(`Log Compaction`)后的消息。

---

## 4.变长字段

Kafka从 `0.11.0` 版本开始所使用的消息格式版本为v2，这个版本的消息相比v0 和v1 的版本而言改动很大，同时还参考了`Protocol Buffer`而引入了变长整型(`Varints`)和ZigZag编码。

回顾 Kafka v0 和v1 版本的消息格式，如果消息本身没有 key，那么 `key length` 字段为-1,int类型的需要 4 个字节来保存，而如果采用 `Varints` 来编码则只需要 1个字节。根据 `Varints` 的规则可以推导出0~63之间的数字占1个字节,64~8191之间的数字占2个字节，8192~1048575之间的数字占3 个字节。而 Kafka broker 端配置 `message.max.bytes` 的默认大小为 1000012(`Varints` 编码占3 个字节)，如果消息格式中与长度有关的字段采用 `Varints` 的编码，那么绝大多数情况下都会节省空间，而v2 版本的消息格式也正是这样做的。

> `Varints` 并非一直会节省空间，一个 int32 最长会占用 5 个字节 (大于默认的 4个字节)，一个 int64 最长会占用 10 个字节 (大于默认的 8 个字节)。

---

## 5.V2版本

v2 版本中消息集称为 `Record Batch`，而不是先前的 `Message Set`，其内部也包含了一条或多条消息，消息的格式参见下图的中部和右部。在消息压缩的情形下，`Record Batch Header`部分(参见下图左部，从 `first offset` 到 `records count` 字段)是不被压缩的，而被压缩的是records字段中的所有内容。生产者客户端中的ProducerBatch对应这里的RecordBatch,而 ProducerRecord 对应这里的 Record。

![](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311191019459.png)

V2版本的消息不仅提供了更多的功能，比如事务，幂等性等等，某些情况下还减少了消息的空间占用，总体性能提升很大。

---



