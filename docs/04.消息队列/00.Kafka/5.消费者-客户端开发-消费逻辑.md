---
title: 消费者-客户端开发-消费逻辑
date: 2023-01-01 00:00:00
tags:
  - Kafka
  - 消息队列
categories:
  - Kafka
description: 消费者-客户端开发-消费逻辑
toc_number: false
author:
  name: huidong.yin
  link: https://huidongyin.github.io
permalink: /pages/6a99e6af-7333-3944-94f1-6a8a3a84a946/
---

一个正常的消费逻辑需要具备以下几个步骤:

1. 配置消费者客户端参数及创建相应的消费者实例。
2. 订阅主题。
3. 拉取消息并消费。
4. 提交消费位点。
5. 关闭消费者实例。

```java
  public static void consume(){
        Properties props=new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,brokerList);
        props.put(ConsumerConfig.GROUP_ID_CONFIG,groupName);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());

        KafkaConsumer<String, String> consumer=new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList(topicName));

        while(true){
            ConsumerRecords<String, String> records=consumer.poll(Duration.ofMillis(100));
            records.forEach(record->System.out.println("Received message: key = "+record.key()+", value = "+record.value()));
        }
}
```

---

## 1.必要的参数配置

在kafka消费者客户端中有4个参数是必填的。

- `bootstrap.servers`:该参数的释义和生产者客户端中的相同，用来指定连接Kafka集群所需的broker地址清单，具体内容形式为`host1: port1，host2:port2` ，可以设置一个或多个地址，中间用逗号隔开，此参数的默认值为""。注意这里并非需要设置集群中全部的broker地址，消费者会从现有的配置中查找到全部的Kafka集群成员。这里设置两个以上的broker地址信息，当其中任意一个宕机时，消费者仍然可以连接到Kafka集群上。
- `group.id`：消费者隶属的消费组的名称，默认值为""。如果设置为空，则会报出异常。一般而言，这个参数需要设置成具有一定的业务意义的名称。
- `key.deserializer和value.deserializer`：与生产者客户端中的`key.serializer和value.serializer`参数对应.消费者从broker端获取的消息格式都是字节数组(byte[)类型，所以需要执行相应的反序列化操作才能还原成原有的对象格式。这两个参数分别用来指定消息中key和value所需反序列化操作的反序列化器，这两个参数无默认值。 **这里必须填写反序列化器类的全限定名，比如示例中的org.apache.kafka.common.serialization.StringDeserializer，单单指定字符串解串器是错误的。**
- `client.id`：这个参数用来设定Kafka消费者对应的客户端id，默认值也为""。如果客户端不设置，则Kafka消费者会自动生成一个非空字符串，内容形式如"consumer-1""consumer-2"，即字符串"consumer-"与数字的拼接。

> 开发中设置配置推荐使用类`ConsumerConfig`。

在配置完必要的参数之后，我们就可以利用它来创建一个消费者实例了。

---

## 2.订阅主题&分区

在创建好消费者之后，就需要为该消费者订阅相关的主题了。一个消费者可以订阅一个或多个主题，前面的代码中我们使用subscribe方法订阅了一个主题，对于这个方法而言,既可以以集合的形式订阅多个主题，也可以以正则表达式的形式订阅特定模式的主题。subscribe的几个重载方法如下:

```java
        void subscribe(Collection<String> topics);


        void subscribe(Collection<String> topics,ConsumerRebalanceListener callback);


        void subscribe(Pattern pattern,ConsumerRebalanceListener callback);


        void subscribe(Pattern pattern);
```

如果前后两次订阅了不同的主题，那么消费者以最后一次的为准。

```java
consumer.subscribe(Arrays.asList(topicl));
consumer.subscribe(Arrays.asList(topic2));
```

上面的示例中，最终消费者订阅的是 topic2，而不是 topicl,也不是 topicl 和 topic2 的并集。

如果消费者采用的是正则表达式的方式`(subscribe(Pattern))`订阅，在之后的过程中，如果有人又创建了新的主题，并且主题的名字与正则表达式相匹配，那么这个消费者就可以消费到新添加的主题中的消息。

> 在 subscribe的重载方法中有一个参数类型是 ConsumerRebalanceListener，这个是用来设置相应的再均衡监听器的。

消费者不仅可以通过`KafkaConsumer.subscribe()`方法订阅主题，还可以直接订阅某些主题特定分区，在 `KafkaConsumer`中还提供了一个 `assign`方法来实现这些功能。

```java
public void assign(Collection<TopicPartition> partitions)
```

这个方法只接受一个参数 partitions，用来指定需要订阅的分区集合。这里补充说明一下 TopicPartition 类，在 Kafka 的客户端中，它用来表示分区。

```java
public final class TopicPartition implements Serializable {
    private static final long serialVersionUID = -613627415771699627L;

    private int hash = 0;
    private final int partition;
    private final String topic;

    public TopicPartition(String topic, int partition) {
        this.partition = partition;
        this.topic = topic;
    }

    public int partition() {
        return partition;
    }

    public String topic() {
        return topic;
    }
}
```

TopicPartition 类只有 2 个属性:`topic` 和`partition`，分别代表分区所属的主题和自身的分区编号，这个类可以和我们通常所说的主题一分区的概念映射起来。

我们将前面代码中的 subscribe方法修改为 `assign` 方法，这里只订阅 `topic-demo` 主题中分区编号为 0 的分区，相关代码如下:

```java
consumer.assign(Arrays.asList(new TopicPartition("topic-demo",0)));
```

如果我们事先并不知道主题中有多少个分区怎么办?`KafkaConsumer`中的`partitionsFor`方法可以用来查询指定主题的元数据信息，`partitionsFor`方法的具体定义如下:

```java
public List<PartitionInfo> partitionsFor(String topic)
```

其中 PartitionInfo 类型即为主题的分区元数据信息，此类的主要结构如下:

```java
public class PartitionInfo {
    private final String topic; //主题
    private final int partition; //分区
    private final Node leader; //leader 所在的broker节点
    private final Node[] replicas; // AR
    private final Node[] inSyncReplicas; //ISR
    private final Node[] offlineReplicas; // OSR
    //省略部分内容...
}
```

通过 `partitionsFor`方法的协助，我们可以通过 `assign`方法来实现订阅主题 (全部分区)的功能。

```java
    List<TopicPartition> partitions=new ArrayList<> 0);
        List<PartitionInfo> partitionInfos=consumer.partitionsFor(topic);
        if(partitionInfos!=null){
            partitionInfos.stream().map(tpInfo->new TopicPartition(tpInfo.topic(),tpInfo.partition())).forEach(partitions::add);
        }
        consumer.assign(partitions);
```

可以使用 `KafkaConsumer` 中的 `unsubscribe`方法来取消主题的订阅。这个方法既可以取消通过 `subscribe(Collection)`方式实现的订阅，也可以取消通过`subscribe(Pattern)`方式实现的订阅，还可以取消通过 `assign(Collection)`方式实现的订阅。

```java
consumer.unsubscribe();
```

如果将`subscribe(Collection)`或 `assign(Collection)`中的集合参数设置为空集合，那么作用等同于 `unsubscribe()`方法。

```java
        consumer.unsubscribe();
        consumer.subscribe(new ArrayList<String>());
        consumer.assign(new ArrayList<TopicPartition>());
```

如果没有订阅任何主题或分区，那么再继续执行消费程序的时候会报出`IllegalStateException`异常。

集合订阅的方式`subscribe(Collection)`、正则表达式订阅的方式 `subscribe(Pattern)`和指定分区的订阅方式 `assign(Collection)`分表代表了三种不同的订阅状态:**AUTO TOPICS**、**AUTO PATTERN**和 **USER ASSIGNED**(如果没有订阅，那么订阅状态为**NONE**)。然而这三种状态是互斥的，在一个消费者中只能使用其中的一种，否则会报出`IlegalStateException`异常。

**通过 subscribe方法订阅主题具有消费者自动再平衡的功能，在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。**当消费组内的消费者增加或减少时,分区分配关系会自动调整，以实现消费负载均衡及故障自动转移。而**通过assign方法订阅分区时，是不具备消费者自动均衡的功能的**，其实这一点从 `assign`方法的参数中就可以看出端倪,两种类型的subscribe都有`ConsumerRebalanceListener`类型参数的方法,而`assign`方法却没有。

---

## 3.反序列化

KafkaProducer有对应的序列化器，那么与此对应的KafkaConsumer就会有反序列化器。Kafka所提供的反序列化器有 `ByteArrayDeserializer ByteBufferDeserializer BytesDeserializer DoubleDeserializer FloatDeserializer IntegerDeserializer ListDeserializer LongDeserializer ShortDeserializer StringDeserializer UUIDDeserializer VoidDeserializer`，这些序列化器也都实现了`Deserializer`接口，与KafkaProducer中提及的`Serializer`接口一样，`Deserializer`接口也有三个方法。

```java
public interface Deserializer<T> extends Closeable {
    //用来配置当前类
    void configure(Map<String, ?> configs, boolean isKey);

    //如果data为null，那么处理的时候直接返回null而不是抛出一个异常。
    T deserialize(String topic, byte[] data);

    //用来关闭当前序列化器。
    void close();
}
```

以`StringDeserializer`为例分析反序列化器的具体实现。

```java
public class StringDeserializer implements Deserializer<String> {
    private String encoding = "UTF8";

    @Override
    public void configure(Map<String, ?> configs, boolean isKey) {
        String propertyName = isKey ? "key.deserializer.encoding" : "value.deserializer.encoding";
        Object encodingValue = configs.get(propertyName);
        if (encodingValue == null)
            encodingValue = configs.get("deserializer.encoding");
        if (encodingValue instanceof String)
            encoding = (String) encodingValue;
    }

    @Override
    public String deserialize(String topic, byte[] data) {
        try {
            if (data == null)
                return null;
            else
                return new String(data, encoding);
        } catch (UnsupportedEncodingException e) {
            throw new SerializationException("Error when deserializing byte[] to string due to unsupported encoding " + encoding);
        }
    }

    @Override
    public void close() {
        // nothing to do
    }
}
```

`configure`方法主要是做字符编码的配置。`deserialize`方法就是简单的把字节数组转化成字符串，如果为空的时候并不会抛出异常，而是返回null。

另外我们还可以自定义反序列化器，主要有两个步骤：自定义反序列化器和指定自定义的反序列化器。不建议使用自定义的序列化和反序列化器，这样会增加生产者和消费者之间的耦合。在Kafka提供的序列化器和反序列化器满足不了当前应用程序的需求前提下，推荐使用 `Avro JSON Thrift ProtoBuf 或 Protostuff`等通用的序列化工具来包装。

---

## 4.消息消费

Kafka中的消息是基于拉模式的。消息的消费一般有两种模式：推模式和拉模式。推模式是服务端主动将消息推送给消费者，而拉模式是消费者主动向服务端发起请求来拉取消息。

Kafka中的消息消费是一个不断轮询的过程，消费者所要做的就是重复的调用`poll`方法，而`poll`方法返回的是所订阅的主题/分区上的一组消息。

对于`poll`方法而言，如果某些分区中没可供消费的消息，那么此分区对应的消息拉取结果就为空；如果订阅的所有分区都没有可供消费的消息，那么`poll`方法返回为空的消息集合。

```java
public ConsumerRecords<K, V> poll(final Duration timeout);
```

`poll`方法里面还有一个超时时间参数timeout，用来控制`poll`方法的阻塞时间，在消费者的缓冲区里没有可用数据的时候会发生阻塞。timeout的设置取决于应用程序对响应速度的要求，比如需要在多长时间内将控制权移交给执行轮询的应用线程。可以直接将timeout设置为0，这样`poll`方法会立刻返回，而不管是否已经拉取到了消息。如果应用线程唯一的工作就是从Kafka中拉取并消费消息，则可以将这个参数设置为`Long.MAX_VALUE`。

消费者消费到的每条消息的类型为`ConsumerRecord`，具体的结构如下：

```java
public class ConsumerRecord<K, V> {
    public static final long NO_TIMESTAMP = RecordBatch.NO_TIMESTAMP;
    public static final int NULL_SIZE = -1;
    public static final int NULL_CHECKSUM = -1;

    private final String topic; //消息所属主题
    private final int partition; //消息所属分区
    private final long offset; //当前消息在所属分区的偏移量
    private final long timestamp; //创建时间或追加到日志文件的时间
    private final TimestampType timestampType; //
    private final int serializedKeySize; //
    private final int serializedValueSize; //
    private final Headers headers; //消息头
    private final K key; //键
    private final V value; //消息体

    private volatile Long checksum; // CRC32的校验值
}
```

我们在消费消息的时候可以直接对`ConsumerRecord`中感兴趣的字段进行具体的业务逻辑处理。

`poll`方法的返回值类型是`ConsumerRecords`它用来表示一次拉取操作所获得的消息集，内部包含了若干`ConsumerRecord`，它提供了一个`iterator`方法来循环遍历消息集内部的消息。

除了上面的方式，我们还可以按照分区维度来进行消费，这一点很重要，在手动提交消费位点的时候特别明显。`ConsumerRecords`类提供了一个`records(TopicPartition)`方法来获取消息集中指定分区的消息。

```java
public List<ConsumerRecord<K, V>>records(TopicPartition partition);
```

下面的案例是按照分区维度消费消息。

```java
ConsumerRecords<String, String> records=consumer.poll(Duration.ofMillis(1000));
records.partitions().stream().flatMap(partition->records.records(partition).stream()).map(record->record.partition()+" : "+record.value()).forEach(System.out::println);
```

上面的`ConsumerRecords.partitions()`方法用来获取消息集合中的所有分区。在ConsumerRecords类中还提供了按照主题维度进行消费的方法，这个方法是`records(TopicPartition)`的重载方法。

```java
public Iterable<ConsumerRecord<K, V>>records(String topic);
```

ConsumerRecords类中并没有提供与`partitions`方法类似的`topics()`方法，如果需要按照主题维度来进行消费，那么只能根据消费者订阅主题时的列表来进行逻辑处理。下面的案例演示了如何使用ConsumerRecords的`record(String topic)`方法。

```java
public static void consume(){
        Properties properties=new Properties();
        properties.put("key.deserializer","org.apache.kafka.common.serialization.StringDeserializer");
        properties.put("value.deserializer","org.apache.kafka.common.serialization.StringDeserializer");
        properties.put("bootstrap.servers",brokerList);
        properties.put("group.id",groupId);
        KafkaConsumer<String, String> consumer=new KafkaConsumer<>(properties);
        List<String> topicList=List.of("topic1","topic2");
        consumer.subscribe(topicList);
        while(true){
            ConsumerRecords<String, String> records=consumer.poll(Duration.ofMillis(1000));
            for(String topic:topicList){
                for(ConsumerRecord<String, String> record:records.records(topic)){
                    System.out.println(record.partition()+" : "+record.value());
                }
            }
        }
}
```

在ConsumerRecords类中还提供了几个方法来方便我们对消息集合进行处理：`count()`方法返回消息集中的消息个数，返回类型为int；`isEmpty()`方法用来判断消息集合是否为空，返回类型是boolean；`empty()`方法用来获取一个空的消息集合，返回类型为`ConsumerRecords<K,V>`。

截止目前，可以简单的认为`poll()`方法只是拉取一下消息而已，实际上它的内部涉及消费位点，消费者协调器，组协调器，消费者的选举，分区分配的分发，再均衡的逻辑，心跳等内容。

---

## 5.控制或关闭消费

KafkaConsumer提供了对消费速度进行控制的方法，在某些场景下我们可能需要临时暂停某些分区的消费而先消费其他分区，当达到一定条件的时候再恢复这些分区的消费。KafkaConsumer中使用 `pause()`和 `resume()` 方法来分别实现暂停某些分区在拉取操作时返回数据给客户端和恢复某些分区向客户端返回数据的操作。

```java
    void pause(Collection<TopicPartition> partitions);

    void resume(Collection<TopicPartition> partitions);
```

KafkaConsumer 还提供了一个无参的 `paused()` 方法来返回被暂停的分区集合。

```java
Set<TopicPartition> paused();
```

如何优雅的退出消费？

1. 使用`while(isRunning.get())`的方式，这样可以通过在其他地方设定`isRunning.set(false)`来退出 while 循环。
2. 调用 KafkaConsumer 的 `wakeup()`方法，`wakeup()`方法是 KafkaConsumer 中唯一可以从其他线程里安全调用的方法(KafkaConsumer 是非线程安全的)，调用 `wakeup()`方法后可以退出 `poll()`的逻辑,并抛出 WakeupException的异常，我们也不需要处理WakeupException 的异常，它只是一种跳出循环的方式。

> 跳出循环以后一定要显式地执行关闭动作以释放运行过程中占用的各种系统资源，包括内存资源、Socket 连接等。

对于`close()`方法，可以指定一个超时参数，如果使用无参的方法，默认的最长等待时间是30s，超过这个时间后会强制退出。

---
