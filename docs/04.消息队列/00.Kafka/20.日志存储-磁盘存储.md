---
title: 日志存储-磁盘存储
date: 2023-11-19 09:40:39
permalink: /pages/74e2cfdd-b28c-38d8-b878-a7ca01951017/
categories:
  - Kafka
tags:
  - Kafka
  - 消息队列
author: 
  name: huidong.yin
  link: https://huidongyin.github.io
---

Kafka依赖于文件系统来存储和缓存消息。对于各个存储介质的速度如图5-20所示的相同，层级越高代表速度越快。很显然，磁盘处于一个比较尴尬的位置，这不禁让我们怀疑 Kafka采用这种持久化形式能否提供有竞争力的性能。在传统的消息中间件RabbitMQ 中就使用内存作为默认的存储介质，而磁盘作为备选介质，以此实现高吞吐和低延迟的特性。然而，事实上磁盘可以比我们预想的要快，也可能比我们预想的要慢，这完全取决于我们如何使用它。

![]()

操作系统可以针对线性读写做深层次的优化，比如预读(`read-ahead`，提前将一个比较大的磁盘块读入内存)和后写(`write-behind`，将很多小的逻辑写操作合并起来组成一个大的物理写操作)技术。顺序写盘的速度不仅比随机写盘的速度快，而且也比随机写内存的速度快，如图5-21所示。

![]()

Kafka 在设计时采用了文件追加的方式来写入消息，即只能在日志文件的尾部追加新的消息，并且也不允许修改已写入的消息，这种方式属于典型的顺序写盘的操作，所以就算Kafka使用磁盘作为存储介质，它所能承载的吞吐量也不容小觑。但这并不是让Kafka在性能上具备足够竞争力的唯一因素。

---

## 1.页缓存

页缓存是操作系统实现的一种主要的磁盘缓存，以此用来减少对磁盘I/O的操作。具体来说，就是把磁盘中的数据缓存到内存中，把对磁盘的访问变为对内存的访问。为了弥补性能上的差异，现代操作系统越来越“激进地”将内存作为磁盘缓存，甚至会非常乐意将所有可用的内存用作磁盘缓存，这样当内存回收时也几乎没有性能损失，所有对于磁盘的读写也将经由统一的缓存。

当一个进程准备读取磁盘上的文件内容时，操作系统会先查看待读取的数据所在的页(page)是否在页缓存(pagecache)中，如果存在(命中)则直接返回数据，从而避免了对物理磁盘的I/O操作;如果没有命中，则操作系统会向磁盘发起读取请求并将读取的数据页存入页缓存，之后再将数据返回给进程。同样，如果一个进程需要将数据写入磁盘，那么操作系统也会检测数据对应的页是否在页缓存中，如果不存在，则会先在页缓存中添加相应的页，最后将数据写入对应的页。被修改过后的页也就变成了脏页，操作系统会在合适的时间把脏页中的数据写入磁盘，以保持数据的一致性。

Linux 操作系统中的`vm.dirty_background_ratio`参数用来指定当脏页数量达到系统内存的百分之多少之后就会触发`pdflush/flush/kdmflush`等后台回写进程的运行来处理脏页，一般设置为小于10的值即可,但不建议设置为0。与这个参数对应的还有一个`vm.dirty_ratio`参数，它用来指定当脏页数量达到系统内存的百分之多少之后就不得不开始对脏页进行处理,在此过程中，新的IO请求会被阻挡直至所有脏页被冲刷到磁盘中。

对一个进程而言，它会在进程内部缓存处理所需的数据，然而这些数据有可能还缓存在操作系统的页缓存中，因此同一份数据有可能被缓存了两次。并且，除非使用Direct I/O的方式,否则页缓存很难被禁止。此外，用过Java的人一般都知道两点事实:对象的内存开销非常大，通常会是真实数据大小的几倍甚至更多，空间使用率低下;Java的垃圾回收会随着堆内数据的增多而变得越来越慢。基于这些因素，使用文件系统并依赖于页缓存的做法明显要优于维护一个进程内缓存或其他结构，至少我们可以省去了一份进程内部的缓存消耗，同时还可以通过结构紧凑的字节码来替代使用对象的方式以节省更多的空间。如此，我们可以在32GB的机器上使用 28GB 至30GB的内存而不用担心GC所带来的性能问题。此外，即使Kafka服务重启，页缓存还是会保持有效，然而进程内的缓存却需要重建。这样也极大地简化了代码逻辑，因为维护页缓存和文件之间的一致性交由操作系统来负责，这样会比进程内维护更加安全有效。 Kafka 中大量使用了页缓存，这是Kafka实现高吞吐的重要因素之一。虽然消息都是先被写入页缓存，然后由操作系统负责具体的刷盘任务的，但在Kafka中同样提供了同步刷盘及间断性强制刷盘(`fsync`)的功能，这些功能可以通过`log.flush.interval. messages`、 `log.flush.interval.ms`等参数来控制。同步刷盘可以提高消息的可靠性，防止由于机器掉电等异常造成处于页缓存而没有及时写入磁盘的消息丢失。不过并不建议这么做，刷盘任务就应交由操作系统去调配，消息的可靠性应该由多副本机制来保障，而不是由同步刷盘这种严重影响性能的行为来保障。

Linux系统会使用磁盘的一部分作为swap分区，这样可以进行进程的调度:把当前非活跃的进程调入 **swap** 分区，以此把内存空出来让给活跃的进程。对大量使用系统页缓存的Kafka而言，应当尽量避免这种内存的交换，否则会对它各方面的性能产生很大的负面影响。我们可以通过修改 `vm.swappiness`参数(Linux系统参数)来进行调节。`vm.swappiness` 参数的上限为 100，它表示积极地使用swap分区，并把内存上的数据及时地搬运到**swap**分区中; `vm.swappiness`参数的下限为0，表示在任何情况下都不要发生交换(`vm.swappiness=0`的含义在不同版本的Linux内核中不太相同，这里采用的是变更后的最新解释)，这样一来，当内存耗尽时会根据一定的规则突然中止某些进程。建议将这个参数的值设置为1，这样保留了**swap**的机制而又最大限度地限制了它对Kafka性能的影响。

---

## 2.磁盘I/O流程

![]()

针对不同的应用场景，I/O调度策略也会影响 IO 的读写性能，目前Linux 系统中的I/O调度策略有4种，分别为**NOOP**、**CFQ**、**DEADLINE** 和 **ANTICIPATORY**，默认为 **CFQ**。

1. NOOP
NOOP 算法的全写为`No Operation`。该算法实现了最简单的FIFO队列，所有I/O请求大致按照先来后到的顺序进行操作。之所以说“大致”，原因是NOOP 在 FIFO的基础上还做了相邻I/O请求的合并，并不是完全按照先进先出出的规则满足I/O请求。假设有如下的 I/O 请求序列:
```
100，500，101，10，56，1000
```

NOOP 将会按照如下顺序满足I/O 请求:

```
100(101)，500，10，56，1000
```

2.CFQ
CFQ算法的全写为`Completely Fair Queuing`。该算法的特点是按照I/O请求的地址进行排序，而不是按照先来后到的顺序进行响应。假设有如下的 I/O 请求序列:

```
100，500，101，10，56，1000
```

CFQ将会按照如下顺序满足:

```
100，101，500，1000，10，56
```

CFQ 是默认的磁盘调度算法,对于通用服务器来说是最好的选择。它试图均匀地分布对I/O带宽的访问。CFQ为每个进程单独创建一个队列来管理该进程所产生的请求，也就是说，每个进程一个队列，各队列之间的调度使用时间片进行调度，以此来保证每个进程都能被很好地分配到I/O带宽。I/O调度器每次执行一个进程的4次请求。在传统的 SAS盘上，磁盘寻道花去了绝大多数的I/O响应时间。CFQ的出发点是对I/O地址进行排序，以尽量少的磁盘旋转次数来满足尽可能多的I/O 请求。在CFQ 算法下，SAS盘的吞吐量大大提高了。相比于 NOOP 的缺点是，先来的I/O 请求并不一定能被满足，可能会出现“饿死”的情况。

3.DEADLINE
DEADLINE 在 CFQ的基础上，解决了I/O请求“饿死”的极端情况。除了CFQ本身具有的I/O排序队列，DEADLINE额外分别为读I/O和写I/O提供了FIFO队列。读 FIFO队列的最大等待时间为500ms，写FIFO队列的最大等待时间为5s。FIFO队列内的I/O请求优先级要比 CFQ队列中的高,而读 FIFO队列的优先级又比写FIFO 队列的优先级高。优先级可以表示如下:

```
FIFO(Read)> FIFO(Write)>CFQ
```

4.ANTICIPATORY
CFQ和DEADLINE考虑的焦点在于满足零散I/O请求上。对于连续的I/O请求，比如顺序读，并没有做优化。为了满足随机I/O和顺序I/O混合的场景，Linux还支持ANTICIPATORY调度算法。ANTICIPATORY在DEADLINE的基础上，为每个读I/O都设置了6ms的等待时间窗口。如果在6ms内OS收到了相邻位置的读I/O请求，就可以立即满足。ANTICIPATORY 算法通过增加等待时间来获得更高的性能，假设一个块设备只有一个物理查找磁头(例如一个单独的 SATA硬盘)，将多个随机的小写入流合并成一个大写入流(相当于将随机读写变顺序读写)，通过这个原理来使用读取/写入的延时换取最大的读取/写入吞吐量。适用于大多数环境，特别是读取/写入较多的环境。

不同的磁盘调度算法(以及相应的I/O优化手段)对Kafka这类依赖磁盘运转的应用的影响很大，建议根据不同的业务需求来测试并选择合适的磁盘调度算法。

从文件系统层面分析，Kafka操作的都是普通文件，并没有依赖于特定的文件系统，但是依然推荐使用EXT4或XFS。尤其是对XFS而言，它通常有更好的性能，这种性能的提升主要影响的是 Kafka 的写入性能。


---

## 3.零拷贝

除了消息顺序追加、页缓存等技术，Kafka还使用零拷贝(Zero-Copy)技术来进一步提升性能。所谓的零拷贝是指将数据直接从磁盘文件复制到网卡设备中，而不需要经由应用程序之手。零拷贝大大提高了应用程序的性能，减少了内核和用户模式之间的上下文切换。对Linux操作系统而言，零拷贝技术依赖于底层的`sendfile()`方法实现。对应于Java语言， `FileChannal.transferTo()`方法的底层实现就是`sendfile()`方法。

单纯从概念上理解“零拷贝”比较抽象，这里简单地介绍一下它。考虑这样一种常用的情形:你需要将静态内容(类似图片、文件)展示给用户。这个情形就意味着需要先将静态内容从磁盘中复制出来放到一个内存buf中，然后将这个buf通过套接字(Socket)传输给用户，进而用户获得静态内容。这看起来再正常不过了，但实际上这是很低效的流程，我们把上面的这种情形抽象成下面的过程:

```
read(file, tmp_buf,len);
write(socket,tmp_buf,len);
```

首先调用read(将静态内容(这里假设为文件A)读取到`tmp_buf`,然后调用`write()`将 `tmp_buf`写入 Socket，如图5-23 所示。

![]()

在这个过程中，文件A经历了4次复制的过程:

1. 调用`read()`时，文件A中的内容被复制到了内核模式下的`Read Buffer` 中。
2. CPU控制将内核模式数据复制到用户模式下。
3. 调用`write()`时，将用户模式下的内容复制至内核模式下的`Socket Buffer` 中。
4. 将内核模式下的`Socket Buffer` 的数据复制至到网卡设备中传送。

从上面的过程可以看出，数据平白无故地也从内核模式到用户模式“走了一圈”，浪费了 2次复制过程:第一次是从内核模式复制到用户户模式;第二次是从用户模式再复制回内核模式，即上面4次过程中的第2步和第3步。而且在在上面的过程中，内核和用户模式的上下文的切换也是4次。

如果采用了零拷贝技术，那么应用程序可可以直接请求内核把磁盘中的数据传输给Socket，如图 5-24 所示。

![]()

零拷贝技术通过`DMA(Direct Memory Acc cess)`技术将文件内容复制到内核模式下的 `Read Buffer` 中。不过没有数据被复制到`Socket Buffer`，相反只有包含数据的位置和长度的信息的文件描述符被加到 `Socket Buffer` 中。`DMA` 引擎直接将数据从内核模式中传递到网卡设备(协议引擎)。这里数据只经历了2次复制就从磁盘中传送出去了，并且上下文切换也变成了2次。零拷贝是针对内核模式而言的，数据在内核模式下实现了零拷贝。

---
