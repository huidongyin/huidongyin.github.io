---
title: 服务端-控制器
date: 2023-01-01 00:00:00
tags: 
  - Kafka
  - 消息队列
categories: 
  - Kafka
description: Kafka入门
toc_number: false
author:
  name: huidong.yin
  link: https://huidongyin.github.io
permalink: /pages/6644d3f3-1342-3a92-88e3-e70a640163bd/
---

在 Kafka集群中会有一个或多个 broker,其中有一个 broker会被选举为控制器(KafkaController),它负责管理整个集群中所有分区和副本的状态。**当某个分区的 leader副本出现故障时,由控制器负责为该分区选举新的 leader副本**。**当检测到某个分区的ISR集合发生变化时,由控制器负责通知所有 broker更新其元数据信息**。**当使用 `kafka-topics.sh`脚本为某个 topic增加分区数量时,同样还是由控制器负责分区的重新分配**。

---

## 1.控制器的选举及异常恢复

Kafka中的控制器选举工作依赖于 ZooKeeper,成功竞选为控制器的 broker会在 ZooKeeper中创建`/controller`这个临时(EPHEMERAL)节点,此临时节点的内容参考如下:

```text
{"version": 1,"brokerid":0,"timestamp":"1529210278988"}
```

其中 `version` 在目前版本中固定为1, `brokerid` 表示成为控制器的 broker的id编号 `timestamp`表示竞选成为控制器时的时间戳。

在任意时刻,集群中有且仅有一个控制器。每个 broker启动的时候会去尝试读取`/controller`节点的 brokerid的值,如果读取到 brokerid的值不为-1,则表示已经有其他 broker节点成功竞选为控制器,所以当前 broker就会放弃竞选;如果 ZooKeeper中不存在`/controller`节点,或者这个节点中的数据异常,那么就会尝试去创建`/controller`节点。当前 broker去创建节点的时候,也有可能其他 broker同时去尝试创建这个节点,只有创建成功的那个 broker才会成为控制器,而创建失败的 broker竞选失败。每个 broker都会在内存中保存当前控制器的 brokerId值,这个值可以标识为 `activeControllerld`。

ZooKeeper中还有一个与控制器有关的`/controller_epoch`节点,这个节点是持久(PERSISTENT)节点,节点中存放的是一个整型的 `controller_epoch`值。 `controller_epoch`用于记录控制器发生变更的次数,即记录当前的控制器是第几代控制器,我们也可以称之为“控制器的纪元”。

`controller_epoch`的初始值为1,即集群中第一个控制器的纪元为1,当控制器发生变更时,每选出一个新的控制器就将该字段值加1。每个和控制器交互的请求都会携带 `controller_epoch这`个字段,如果请求的 `controller_epoch`值小于内存中的 `controller_epoch`值,则认为这个请求是向已经过期的控制器所发送的请求,那么这个请求会被认定为无效的请求。如果请求的 `controller_epoch值`大于内存中的 `controller_epoch`值,那么说明已经有新的控制器当选了。由此可见,Kafka通过 `controller_epoch`来保证控制器的唯一性,进而保证相关操作的一致性。

具备控制器身份的 broker 需要比其他普通的 broker 多一份职责,具体细节如下:

1. 监听分区相关的变化。为 ZooKeeper中的`/admin/reassign_partitions`节点注册 PartitionReassignmentHandler,用来处理分区重分配的动作。为 ZooKeeper中的`/isr_change_notification`节点注册 IsrChangeNotificetionHandler,用来处理ISR集合变更的动作。为 ZooKeeper中的`/admin/preferred-replica-election`节点添加 PreferredReplicaElectionHandler,用来处理优先副本的选举动作。
2. 监听主题相关的变化。为 ZooKeeper中的`/brokers/topics`节点添加Topic ChangeHandler,用来处理主题增减的变化;为 ZooKeeper中的`/admin/delete_topics`节点添加 TopicDeletionHandler,用来处理删除主题的动作。
3. 监听 broker相关的变化。为ZooKeeper中的`/brokers/ids`节点添加 BrokerChangeHandler,用来处理 broker增减的变化。
4. 从 ZooKeeper中读取获取当前所有与主题、分区及 broker有关的信息并进行相应的管理。对所有主题对应的 ZooKeeper中的`/brokers/topics/<topic>`节点添加PartitionModificationsHandler,用来监听主题中的分区分配变化。
5. 启动并管理分区状态机和副本状态机。
6. 更新集群的元数据信息。
7. 如果参数`auto.leader.rebalance.enable`设置为true,则还会开启一个名为`auto-leader-rebalance-task`的定时任务来负责维护分区的优先副本的均衡。

控制器在选举成功之后会读取 ZooKeeper中各个节点的数据来初始化上下文信息(ControllerContext),并且需要管理这些上下文信息。比如为某个主题增加了若干分区,控制器在负责创建这些分区的同时要更新上下文信息,并且需要将这些变更信息同步到其他普通的broker节点中。不管是监听器触发的事件,还是定时任务触发的事件,或者是其他事件(比如ControlledShutdown)都会读取或更新控制器中的上下文信息,那么这样就会涉及多线程间的同步。如果单纯使用锁机制来实现,那么整体的性能会大打折扣。针对这一现象, Kafka的控制器使用单线程基于事件队列的模型,将每个事件都做一层封装,然后按照事件发生的先后顺序暂存到LinkedBlockingQueue中,最后使用一个专用的线程(ControllerEventThread)按照FIFO(First Input First Output,先入先出)的原则顺序处理各个事件,这样不需要锁机制就可以在多线程间维护线程安全,具体可以参考下图。

在Kafka的早期版本中,并没有采用 KafkaController这样一个概念来对分区和副本的状态进行管理,而是依赖于 ZooKeeper,每个 broker都会在 ZooKeeper上为分区和副本注册大量的监听器(Watcher)。当分区或副本状态变化时,会唤醒很多不必要的监听器,这种严重依赖ZooKeeper的设计会有脑裂、羊群效应,以及造成 Zookeeper过载的隐患。在目前的新版本的设计中,只有 KafkaController在 ZooKeeper上注册相应的监听器,其他的 broker极少需要再监听 ZooKeeper中的数据变化,这样省去了很多不必要的麻烦。不过每个broker还是会对`/controller`节点添加监听器,以此来监听此节点的数据变化(ControllerChangeHandler)。

![](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311240911401.png)

当`/controller`节点的数据发生变化时,每个broker都会更新自身内存中保存的`activeControllerld`。如果broker在数据变更前是控制器,在数据变更后自身的 brokerid值与新的activeControllerId值不一致,那么就需要“**退位**”,关闭相应的资源,比如关闭状态机、注销相应的监听器等。有可能控制器由于异常而下线,造成`/controller`这个临时节点被自动删除;也有可能是其他原因将此节点删除了。

当`/controller`节点被删除时,每个broker都会进行选举,如果broker在节点被删除前是控制器,那么在选举前还需要有一个“退位”的动作。如果有特殊需要,则可以手动删除`/controller`节点来触发新一轮的选举。当然关闭控制器所对应的 broker,以及手动向`/controller`节点写入新的 brokerid的所对应的数据,同样可以触发新一轮的选举。

---

## 2.优雅关闭

如何优雅地关闭Kafka?在做测试的时候经常性使用`jps(或者ps ax)`配合`kill -9`的方式来快速关闭 Kafka broker的服务进程,显然`kill-9`这种“强杀”的方式并不够优雅,它并不会等待 Kafka进程合理关闭一些资源及保存一些运行数据之后再实施关闭动作。在有些
场景中,用户希望主动关闭正常运行的服务,比如更换硬件、操作系统升级、修改 Kafka配置等。如果依然使用上述方式关闭就略显粗暴。

那么合理的操作应该是什么呢?Kafka自身提供了一个脚本工具,就是存放在其bin目录下的 `kafka-server-stop.sh`,这个脚本的内容非常简单,具体内容如下:

```bash
PIDS=$(ps ax I grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '{print $1}')
if [ -z "$PIDS"]; then
    echo "No kafka server to stop "
    exit 1
else
    kill -s TERM $PIDS
fi
```

可以看出 `kafka-server-stop.sh`首先通过ps ax的方式找出正在运行Kafka的进程号PIDS,然后使用`kill -s TERM $PIDS`的方式来关闭。但是这个脚本在很多时候并不奏效,这一点与ps命令有关系。在 Linux操作系统中,ps命令限制输出的字符数不得超过页大小 PAGE_SIZE,一般CPU的内存管理单元(Memory Management Unit,简称MMU)的PAGE_SIZE为4096。也就是说,ps命令的输出的字符串长度限制在4096内,这会有什么问题呢?

Kafka进程有关的输出信息太长,所以 `kafka-server-stop.sh`脚本在很多情况下并不会奏效。

> Kafka服务启动的入口就是 `kafka.Kafka`,采用 Scala语言编写 `object`。

那么怎么解决这种问题呢?

这里我们可以直接修改 kafka-server-stop.sh脚本的内容,将其中的第一行命令修改如下:
`PIDS=$(ps ax | grep -i 'kafka' | grep java | grep -v grep | awk '{print $1}')`

即把`\.Kafka`去掉,这样在绝大多数情况下是可以奏效的。如果有极端情况,即使这样也不能关闭,那么只需要按照以下两个步骤就可以优雅地关闭Kafka的服务进程:

1. 获取Kafka的服务进程号PIDS。可以使用Java中的jps命令或使用 Linux系统中的ps命令来查看。
2. 使用`kill -s TERM $PIDS或kill -15 $PIDS`的方式来关闭进程,注意千万不要使用`kill -9`的方式。

为什么这样关闭的方式会是优雅的? Kafka服务入口程序中有一个名为`kafka-shutdown-hock`的关闭钩子,待Kafka进程捕获终止信号的时候会执行这个关闭钩子中的内容,其中除了正常关闭一些必要的资源,还会执行一个控制关闭(ControlledShutdown)的动作。**使用ControlledShutdown的方式关闭Kafka有两个优点:一是可以让消息完全同步到磁盘上,在服务下次重新上线时不需要进行日志的恢复操作;二是ControllerShutdown在关闭服务之前,会对其上的 leader副本进行迁移,这样就可以减少分区的不可用时间**。

若要成功执行ControlledShutdown动作还需要有一个先决条件,就是参数 `controlled.shutdown.enable` 的值需要设置为true,不过这个参数的默认值就为true,即默认开始此项功能。ControlledShutdown动作如果执行不成功还会重试执行,这个重试的动作由参数
`controlled.shutdown.max.retries`配置,默认为3次,每次重试的间隔由参数`controlled.shutdown.retry.backoff.ms`设置,默认为5000ms。


下面是ControlledShutdown的整个执行过程。

如图所示,假设此时有两个 broker,其中待关闭的 broker的id为x, Kafka控制器所对应的 broker的id为y。待关闭的 broker在执行 ControlledShutdown动作时首先与Kafka控制器建立专用连接(对应图中的步骤①),然后发送 ControlledShutdownRequest请求,ControlledShutdownRequest请求中只有一个 brokerid字段,这个 brokerid字段的值设置为自身的 brokerid的值,即x(对应图中的步骤②)。

Kafka控制器在收到ControlledShutdownRequest请求之后会将与待关闭broker有关联的所有分区进行专门的处理,这里的“有关联”是指分区中有副本位于这个待关闭的 broker之上(这里会涉及Kafka控制器与待关闭 broker之间的多次交互动作,涉及leader副本的迁移和副本的关闭动作,对应图中的步骤③)。

![](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311240914463.png)


如果这些分区的副本数大于1且leader副本位于待关闭broker上,那么需要实施leader副本的迁移及新的SR的变更。具体的选举分配的方案由专用的选举器ControlledShutdownLeaderSelector提供。

如果这些分区的副本数只是大于1, leader副本并不位于待关闭broker上,那么就由Kafka控制器来指导这些副本的关闭。如果这些分区的副本数只是为1,那么这个副本的关闭动作会在整个 ControlledShutdown动作执行之后由副本管理器来具体实施。

对于分区的副本数大于1且 leader副本位于待关闭 broker上的这种情况,如果在Kafka控制器处理之后 leader副本还没有成功迁移,那么会将这些没有成功迁移 leader副本的分区记录下来,并且写入 ControlledShutdownResponse的响应(对应图中的步骤④,整个ControlledShutdown动作是一个同步阻塞的过程)。

待关闭的 broker在收到 ControlledShutdownResponse响应之后,需要判断整个ControlledShutdown动作是否执行成功,以此来进行可能的重试或继续执行接下来的关闭资源的动作。执行成功的标准是 ControlledShutdownResponse中 `error_code` 字段值为0,并且 `partitions_remaining`数组字段为空。

> 图中也有可能`ⅹ=y`,即待关闭的 broker 同时是Kafka控制器,这也就意味着自己可以给自己发送ControlledShutdownRequest请求,以及等待自身的处理并接收ControlledShutdownResponse的响应,具体的执行细节和`x!=y`的场景相同。

在了解了整个ControlledShutdown动作的具体细节之后,我们不难看出这一切实质上都是由ControlledShutdownRequest请求引发的,我们完全可以自己开发一个程序来连接Kafka控制器,以此来模拟对某个broker实施ControlledShutdown的动作。为了实现方便,我们可以对KafkaAdminClient做一些扩展来达到目的。

> ControlledShutdown只是关闭 Kafka broker的一个中间过程,所以不能寄希望于只使用ControlledShutdownRequest请求就可以关闭整个 Kafka broker的服务进程。

---

## 3.分区leader的选举

分区leader副本的选举由控制器负责具体实施。当创建分区(创建主题或增加分区都有创建分区的动作)或分区上线(比如分区中原先的 leader副本下线,此时分区需要选举一个新的leader上线来对外提供服务)的时候都需要执行 leader的选举动作,对应的选举策略为OfflinePartitionLeaderElectionStrategy。**这种策略的基本思路是按照AR集合中副本的顺序査找第一个存活的副本,并且这个副本在ISR集合中**。一个分区的AR集合在分配的时候就被指定,并且只要不发生重分配的情况,集合内部副本的顺序是保持不变的,而分区的ISR集合中副本的顺序可能会改变。

注意这里是根据AR的顺序而不是ISR的顺序进行选举的。举个例子,集群中有3个节点:
broker0、 broker1和 broker2,在某一时刻具有3个分区且副本因子为3的主题 `topic-leader` 的具体信息如下:

```text
./kafka-topic.sh --zookeeper localhost:2181/kafka --describe --topic topic-leader
```

此时关闭 broker0,那么对于分区2而言,存活的AR就变为[1,2],同时ISR变为[2,1]。此时查看主题 `topic-leader` 的具体信息(参考如下),分区2的 leader就变为了1而不是2。

```text
./kafka-topic.sh --zookeeper localhost:2181/kafka --describe --topic topic-leader
```

如果ISR集合中没有可用的副本,那么此时还要再检查一下所配置的 `unclean.leader.election.enable`参数(默认值为 false)。如果这个参数配置为tue,那么表示允许从非ISR列表中的选举 leader,从AR列表中找到第一个存活的副本即为leader。当分区进行重分配的时候也需要执行 leader的选举动作,对应的选举策略为 ReassignPartitionLeaderElectionStrategy。这个选举策略的思路比较简单:**从重分配的AR列表中找到第一个存活的副本,且这个副本在目前的ISR列表中**。

当发生优先副本的选举时,直接将优先副本设置为 leader即可,AR集合中的第一个副本即为优先副本(PreferredReplicaPartitionLeaderElectionStrategy)。

还有一种情况会发生 leader的选举,当某节点被优雅地关闭(也就是执行ControlledShutdown)时,位于这个节点上的leader副本都会下线,所以与此对应的分区需要执行 leader的选举。与此对应的选举策略(ControlledShutdownPartitionStrategy)为:**从AR列表中找到第一个存活的副本,且这个副本在目前的ISR列表中,与此同时还要确保这个副本不处于正在被关闭的节点上**。

---



