---
title: 源码分析-Kafka源码构建&运行
date: 2023-01-01 00:00:00
tags:
    - Kafka
    - 消息队列
categories:
    - Kafka
description: 源码分析-Kafka源码构建&运行
toc_number: false
author:
name: huidong.yin
link: https://huidongyin.github.io
permalink: /pages/f6675ee1-3c67-3cd9-94da-4acd80af9fb2/

---

## 1.安装Scala

```text
brew install scala@2.12
```

安装成功有一段很重要的话：

```text
To use with IntelliJ, set the Scala home to:
  /opt/homebrew/opt/scala@2.12/idea

scala@2.12 is keg-only, which means it was not symlinked into /opt/homebrew,
because this is an alternate version of another formula.

If you need to have scala@2.12 first in your PATH, run:
  echo 'export PATH="/opt/homebrew/opt/scala@2.12/bin:$PATH"' >> ~/.zshrc
```

---

## 2.安装JDK8

[下载地址:https://www.azul.com/downloads/?package=jdk#download-openjdk](https://www.azul.com/downloads/?package=jdk#download-openjdk)

---

## 3.安装Gradle

[下载地址：https://gradle.org/releases/](https://gradle.org/releases/)

配置环境变量
```text
echo 'export PATH="/Users/huidong/gradle-6.9.2/bin:$PATH"' >> ~/.zshrc
```

---

## 4.下载kafka的源码包

[下载地址:https://github.com/apache/kafka/releases/tag/2.7.2](https://github.com/apache/kafka/releases/tag/2.7.2)

命令下载：

```text
wget https://github.com/apache/kafka/archive/refs/tags/2.7.2.tar.gz
```

---

## 5.构建源码

1. 将下载好的Kafka源码导入idea。
2. 在项目根目录下执行命令`gradle`。
3. 接下来执行`./gradlew jar`。
4. 接下来执行`./gradlew idea`。

命令说明：

- `gradle`: 构建并更新 `gradle wrapper` 插件。
- `./gradlew jar`:构建jar包。
- `./gradlew idea`:把Kafka源码导入到Idea中。

至此，Kafka源码环境构建完成。

---

## 6.启动Kafka源码

### 6.1 broker启动

- 找到`core`目录下的`Kafka.scala`。
- 启动这个文件。
- 配置启动参数：
    - VM：`-Dkafka.logs.dir=./logs`
    - 应用程序参数：`config/server.properties`

如图所示：

![](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/Kafka/202312231339518.png)

---

### 6.2 broker启动日志

默认情况下，使用代码启动broker没有日志输出，应该如何解决呢？

**SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder"**

在`build.gradle`的子项目`core`中需要添加这两个依赖:

```text
project(':core') {
  println "Building project 'core' with Scala version ${versions.scala}"
  ...

  dependencies {
    ...
    // 添加以下两个依赖
    // https://mvnrepository.com/artifact/org.slf4j/slf4j-api
    compile group: 'org.slf4j', name: 'slf4j-api', version: '1.7.25'
    // https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12
    compile group: 'org.slf4j', name: 'slf4j-log4j12', version: '1.7.25'
}
```

这两行首先要添加`slf4j`的依赖，还要添加基于`slf4j`的`log4j12`的依赖。注意这里一定要用`compile`而不是`testCompile`，因为后者的意思是只在test环境下编译.

----

### 6.3 启动客户端

在example包下导入Java文件。

```java
package kafka.examples.huidong;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringDeserializer;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class ProducerTest {


    public static void main(String[] args) {
        new Thread(ProducerTest::consume).start();
        produce();
    }

    public static void consume() {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9002");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "code-group-debug");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList("code-topic-debug"));

        while (true) {
             ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
            records.forEach(record -> System.out.println("Received message: key = " + record.key() + ", value = " + record.value()));
        }
    }

    public static void produce() {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9002");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");

        KafkaProducer<String, String> producer = new KafkaProducer<>(props);

        ProducerRecord<String, String> record = new ProducerRecord<>("code-topic-debug",  "value");

        for (int i = 0; i < 1; i++) {
            producer.send(record, (metadata, exception) -> {
                if (exception == null) {
                    System.out.println("Message sent to partition " + metadata.partition() + " with offset " + metadata.offset());
                } else {
                    exception.printStackTrace();
                }
            });
        }

        producer.close();
    }


}
```
设置启动该类的JVM环境变量：`-Dkafka.logs.dir=./logs`，然后启动该类。

---

## 7.Kafka目录结构

- `bin`:保存Kafka命令行脚本。
- `checkstyle`:代码规范，自动化检测。
- `clients`:保存Kafka客户端代码。
- `config`:保存Kafka的配置文件，比较重要的，例如`server.properties`。
- `connect`:保存Connect组件的源代码。Kafka Connect 组件是用来实现Kafka与外部系统之间的实时数据传输的。
- `core`:保存全部的broker端代码。
- `docs`:Kafka 设计文档以及组件相关结构图。
- `examples`:Kafka 样例相关目录。
- `generator`:用于创建测试数据或模拟生产者行为。
- `gradle`:gradle 的脚本和依赖包定义等相关文件。
- `jmh-benchmarks`:Kafka 代码微基准测试相关类。
- `licenses`:包含了项目所使用的开源许可证的相关信息和文件。
- `log4j-appender`:这个目录里面就一个 KafkaLog4jAppender 类。
- `raft`:raft 一致性协议相关模块。
- `streams`:保存Streams组件的源代码。Kafka Streams 是实现Kafka实时流处理的组件。
- `tests`:此目录的内容介绍如何进行 Kafka 系统集成和性能测试。
- `tools`:工具类。
- `vagrant`:介绍如何在 Vagrant 虚拟环境中运行 Kafka，提供了相关的脚本文件和说明文档。

---

