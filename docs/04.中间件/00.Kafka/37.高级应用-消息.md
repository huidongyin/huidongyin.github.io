---
title: 高级应用-消息
date: 2023-01-01 00:00:00
tags:
    - Kafka
    - 消息队列
categories:
    - Kafka
description: 高级应用-消息
toc_number: false
author:
name: huidong.yin
link: https://huidongyin.github.io
permalink: /pages/e923632b-cefb-324f-8e00-ba7c2f6f1073/

---

## 1.消息路由

消息路由是消息中间件中常见的一个概念， 比如在典型的消息中间件Rabbit MQ中就使用路由键Routing Key来进行消息路由。如图所示， Rabbit MQ中的生产者将消息发送到交换器Exchange中， 然后由交换器根据指定的路由键来将消息路由到一个或多个队列中， 消费者消费的是队列中的消息。从整体上而言， Rabbit MQ通过路由键将原本发往一个地方的消息做了区分，然后让不同的消息者消费到自己要关注的消息。

![11-9](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/Kafka/202312192340915.png)

Kafka默认按照主题进行路由， 也就是说， 消息发往主题之后会被订阅的消费者全盘接收，这里没有类似消息路由的功能来将消息进行二级路由，这一点从逻辑概念上来说并无任何问题。从业务应用上而言，如果不同的业务流程复用相同的主题，就会出现消息接收时的混乱，这种问题可以从设计上进行屏蔽，如果需要消息路由，那么完全可以通过细粒度化切分主题来实现。

除了设计缺陷， 还有一些历史遗留的问题迫使我们期望Kafka具备一个消息路由的功能。如果原来的应用系统采用了类似Rabbit MQ这种消息路由的生产消费模型， 运行一段时间之后又需要更换为Kafka， 并且变更之后还需要保留原有系统的编程逻辑。对此， 我们首先需要在这个整体架构中做一层关系映射， 如图所示。这里将Kafka中的消费组与Rabbit MQ中的队列做了一层映射， 可以根据特定的标识来将消息投递到对应的消费组中， 按照Kafka中的术语来讲，消费组根据消息特定的标识来获取消息，其余的都可以被过滤。

![11-10](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/Kafka/202312192346899.png)

具体的实现方式可以在消息的headers字段中加入一个键为`routing key`、值为特定业务标识的Header， 然后在消费端中使用拦截器挑选出特定业务标识的消息。Kafka中消息路由的实现架构如图所示， 消费组Consumer Group 根据指定的Header标识`rk 2`和`rk 3`来消费主题Topic A和Topic B中所有对应的消息而忽略Header标识为`rk 1`的消息， 消费组Consumer Group 2 正好相反。

![11-11](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/Kafka/202312192355473.png)

这里只是演示作为消息中间件家族之一的Kafka如何实现消息路由的功能， 不过消息路由在Kafka的使用场景中很少见， 如无特殊需要， 也不推荐刻意地使用它。

---

## 2.消息轨迹

在使用消息中间件时，我们时常会遇到各种问题：消息发送成功了吗?为什么发送的消息在消费端消费不到?为什么消费端重复消费了消息?对于此类问题，我们可以引入消息轨迹来解决。消息轨迹指的是一条消息从生产者发出， 经由broker存储， 再到消费者消费的整个过程中， 各个相关节点的状态、时间、地点等数据汇聚而成的完整链路信息。生产者、broker、消费者这3个角色在处理消息的过程中都会在链路中增加相应的信息，将这些信息汇聚、处理之后就可以查询任意消息的状态，进而为生产环境中的故障排除提供强有力的数据支持。

对消息轨迹而言，最常见的实现方式是封装客户端，在保证正常生产消费的同时添加相应的轨迹信息埋点逻辑。无论生产，还是消费，在执行之后都会有相应的轨迹信息，我们需要将这些信息保存起来。这里可以参考Kafka中的做法， 它将消费位点信息保存在主题 `__consumer_offset` 中。对应地， 我们同样可以将轨迹信息保存到Kafka的某个主题中， 比如下图中的主题`trace_topic`。

![11-12](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/Kafka/202312202126473.png)

生产者在将消息正常发送到用户主题`real_topic`之后(或者消费者在拉取到消息消费之后)会将轨迹信息发送到主题`trace_topic`中。这里有两种发送方式：第一种是直接通过KafkaProducer发送， 为了不对普通的消息发送造成影响， 可以采取“低功耗”的(比如异步、acks=0等) 发送配置，不过有可能会造成轨迹信息的丢失。另一种方式是将轨迹信息保存到本地磁盘，然后通过某个传输工具(比如Flume) 来同步到Kafka中， 这种方式对正常发送/消费逻辑的影响较小、可靠性也较高，但是需要引入额外的组件，增加了维护的风险。

消息轨迹中包含生产者、broker和消费者的消息， 但是上图中只提及了生产者和消费者的轨迹信息的保存而并没有提及broker信息的保存。生产者在发送消息之后通过确认信息(ProduceRequest对应的响应ProduceResponse)来得知是否已经发送成功，而在消费端就更容易辨别一条消息是消费成功了还是失败了，对此我们可以通过客户端的信息反推出broker的链路信息。当然我们也可以在broker中嵌入一个前置程序来获得更多的链路信息，比如消息流入时间、消息落盘时间等。不过在broker内嵌前置程序， 如果有相关功能更新， 难免需要重启服务，如果只通过客户端实现消息轨迹，则可以简化整体架构、灵活部署。

一条消息对应的消息轨迹信息所包含的内容(包含生产者和消费者)如下所示。

| 角色   | 信息项                 | 含义                                                         |
| ------ | ---------------------- | ------------------------------------------------------------ |
| 生产者 | 消息ID                 | 能够唯一标识一条消息，在查询检索页面可以根据这个消息ID进行精准检索。 |
| 生产者 | 消息Key                | 消息中的Key字段                                              |
| 生产者 | 发送时间               | 消息发送的时间，指生产者的本地时间                           |
| 生产者 | 发送耗时               | 消息发送的时长，从调用`send()`方法开始到服务端返回的总耗时。 |
| 生产者 | 发送状态               | 成功还是失败                                                 |
| 生产者 | 发送的目的地址         | Kafka集群地址，为broker准备的链路信息                        |
| 生产者 | 消息的主题             | 主题，为broker准备的链路信息                                 |
| 生产者 | 消息的分区             | 分区，为broker准备的链路信息                                 |
| 生产者 | 生产者的IP             | 生产者本地的IP地址                                           |
| 生产者 | 生产者的ID             | 生产者的唯一标识，可以用`client.id`替代                      |
| 生产者 | 用户自定义信息（Tags） | 用户自定义的一些附加属性，方便后期检索                       |
| 消费者 | 消息ID                 | 能够唯一标识一条消息，在查询检索页面可以根据这个消息ID进行精准检索。 |
| 消费者 | 消息Key                | 消息中的Key字段                                              |
| 消费者 | 接收时间               | 拉取到消息的时间，指消费者本地的时间                         |
| 消费者 | 消费耗时               | 消息消费的时长，从拉取到消息到业务处理完这条消息的总耗时      |
| 消费者 | 消费状态               | 消费成功或消费失败                                           |
| 消费者 | 重试次数               | 第几次重试消费                                               |
| 消费者 | 消费的源地址           | Kafka集群的地址，为broker准备的链路信息                      |
| 消费者 | 消息的主题             | 主题，为broker准备的链路信息                                 |
| 消费者 | 消息的分区             | 分区，为broker准备的链路信息                                 |
| 消费者 | 消费组             | 消费组的名称                                 |
| 消费者 | 消费者的IP             | 消费者本地IP地址                                |
| 消费者 | 消费者的ID            | 消费者的唯一标识，可以用`client.id`替代                                 |
| 消费者 | 用户自定义信息（Tags）   | 用户自定义的一些附加属性，方便后期检索                                 |


轨迹信息保存到主题`trace_topic`之后， 还需要通过一个专门的处理服务模块对消息轨迹进行索引和存储，方便有效地进行检索。在查询检索页面进行检索的时候可以根据具体的消息ID进行精确检索， 也可以根据消息的key、主题、发送/接收时间进行模糊检索， 还可以根据用户自定义的Tags信息进行有针对性的检索， 最终查询出消息的一条链路轨迹。

---

## 3.消息审计

消息审计是指在消息生产、存储和消费的整个过程之间对消息个数及延迟的审计，以此来检测是否有数据丢失、是否有数据重复、端到端的延迟又是多少等内容。

目前与消息审计有关的产品也有多个， 比如Chaperone(Uber) 、Confluent Control Center、Kafka Monitor(Linked In) ， 它们主要通过在消息体(value字段) 或在消息头(headers字段)中内嵌消息对应的时间戳`timestamp`或全局的唯一标识ID(或者是两者兼备) 来实现消息的审计功能。

内嵌`timestamp`的方式主要是设置一个审计的时间间隔`time_bucket_interval`(可以自定义设置几秒或几分钟) ， 根据这个`time_bucket_interval`和消息所属的`timestamp`来计算相应的时间桶(`time bucket`) 。

算法1：`timestamp-timestamp%time_bucket_interval`(这个算法在时间轮里也有提及)
算法2：`(long) Math.floor((timestamp/time_bucket_interval)*time_bucket_interval)`

根据上面的任意一种算法可以获得`time_bucket`的起始时间`time_bucket_start`， 那么这个`time_bucket`的时间区间可以记录为`(time_bucket_start， time_bucket_start+time_bucket_interval]` ，注意是左闭右开区间。每发送一条或消费一条消息， 可以根据消息中内嵌的`timestamp`来计算并分配到相应的`time_bucket`中， 然后对桶进行计数并存储， 比如可以简单地存储到`Map<long time_bucket_start，long count>`中。

内嵌ID的方式就更加容易理解了，对于每一条消息都会被分配一个全局唯一标识ID，这个和消息轨迹中的消息ID是同一个东西。如果主题和相应的分区固定，则可以为每个分区设置一个全局的ID。当有消息发送时，首先获取对应的ID，然后内嵌到消息中，最后才将它发送到broker中。消费者进行消费审计时， 可以判断出哪条消息丢失、哪条消息重复。

如果还要计算端到端延迟， 那么就需要在消息中内嵌`timestamp`， 也就是消息中同时含有ID和timestamp。

消息审计的实现模型也和消息轨迹的类似， 同样是通过封装自定义的SDK来实现的。图中展示的是**Confluent Control Center**的消息审计的实现模型， 它通过生产者客户端和消费者客户端的拦截器来实现审计信息的保存， 这里的审计信息同样保存到Kafka中的某个主题中，最后通过Confluent Control Center进行最终的信息处理和展示。如果需要类似消息审计的功能，不妨参照此类的实现。

![](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/Kafka/202312202215915.png)

---

## 4.消息代理

**Kafka REST Proxy**可以为Kafka集群提供一系列的REST API接口， 通过这些REST API接口可以在不使用Kafka原生的私有协议或和语言相关的客户端的情况下实现包括发送消息、消费消息、查看集群的状态和执行管理类操作等功能。

从整体设计上来说， **Kafka REST Proxy**预期要实现的是生产者客户端、消费者客户端和命令行工具所提供的所有功能。就目前而言， **Kafka REST Proxy**已经支持大部分功能， 但还有上升空间。官网资料显示目前已经支持的内容包括：

1. 元数据。可以查看集群中大多数的元数据信息， 包括broker、主题、分区， 以及对应的配置信息。
2. 消息发送。可以支持往指定主题或分区中发送消息。Kafka REST Proxy将多个生产者实例进行池化，待监听到用户发送消息的请求之后，会先将消息缓存起来，然后由生产者实例池来执行最后发送至Kafka集群中的请求。
3. 消费。消费者是有状态的，因为它要记录和提交消费位移，所以消费者实例需要和指定的Kafka REST Proxy实例捆绑起来以维持状态。消费位移既可以自动提交， 也可以手动提交。目前限定一个消费者实例一个线程， 用户可以使用多个消费者实例来提高吞吐量。Kafka REST Proxy支持旧版的消费者客户端， 对应的API版本为v 1， 同时支持新版的消费者客户端(KafkaConsumer) ， 对应的API版本为v 2。
4. 数据格式。Kafka REST Proxy用来进行读写的数据都是JSON， 而JSON内部的数据使用Base 64或Avro进行编码。如果采用Avro则需要再引入Confluent公司的另一个组件SchemaRegistry。
5. 集群化部署与负载均衡。Kafka REST Proxy通过多实例部署来提高可靠性和可用性，以此提高系统的负载能力。还可以有效地运用各种负载均衡机制来提供负载均衡的保障。

目前**Kafka REST Proxy**处于规划中的内容包括：
1. 支持管理类操作。例如，创建、删除主题之类的操作，要实现这些功能还需要涉及一些安全机制。
2. 支持多主题的生产请求。目前Kafka REST Proxy只能指定一个主题或一个分区来发送消息。大多数情况下，并不需要多主题的生产请求，这个可以通过发送多个单主题的生产请求来“婉转”实现。然而，如果能将多个单主题的请求组合成单个多主题的请求，则可以减少HTTP请求的次数， 进而可以提升一定的性能。
3. 增加生产消费的配置。目前只有部分Kafka客户端的参数可以在Kafka REST Proxy中被重载，未来将支持重载更多的参数来提高灵活性。

---


