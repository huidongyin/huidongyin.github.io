---
title: 路由计算&数据索引&高效搜索之道
date: 2021年9月11日22:50:43
permalink: /pages/1ed3a878-1ee2-33cc-b3d6-7c54430b7f1d/
tags:
  - 搜索引擎
  - ElasticSearch
author:
  name: huidong.yin
  link: https://huidongyin.github.io
categories:
  - ElasticSearch
---

当数据的规模和复杂性不断增长，搜索和分析变得越来越关键。Elasticsearch（简称为ES）作为一款强大的分布式搜索和分析引擎，正因其卓越的性能和灵活性，成为了数据驱动决策的不可或缺的工具。在ES的不断演进中，版本7.x为我们带来了更先进的功能和更高效的操作方式。

本文将深入探讨Elasticsearch 7.x版本中的核心机制：路由计算原理、数据索引流程和数据搜索流程。我们将揭示其背后的技术内幕，从数据的存储到查询，一步步解析ES如何在分布式环境中实现高效的数据管理和检索。无论您是初次接触Elasticsearch，还是已经熟悉其使用，本文都将带您深入了解ES 7.x版本的精髓，为您在实际应用中发挥其最大威力提供有力支持。

---

## 1.路由计算
在需要将数据分布到不同节点时，我们必须采用映射规则将文档ID映射到特定节点，从而能够高效地定位文档位置。

1. **随机算法：** 这种方法将数据随机分布到各个分片中。然而，在查询时由于无法确定文档所在的分片，需要遍历所有分片来查找数据。这可能导致性能下降。

2. **中心节点维护映射：** 在此方法中，中心节点负责维护数据路由映射关系。然而，这存在单点故障的风险，尤其是在处理大量数据时，维护成本高且效率低下。此外，数据迁移可能会变得复杂。

3. **通过计算路由key：** 这是一种更简单有效的方式。通过对路由key的值进行计算，可以快速地路由到相应的分片。ES的路由算法基于文档ID和routing key来确定分片ID，计算公式为：
 
```bash
shard_number = hash(_routing) % number_of_primary_shards
```

在新增数据时，可以在请求中指定_routing，如下所示：
```bash
 PUT order/_doc/doc_id?routing=routing_key
 {
     "name":"xiaomi 11",
     "id":"doc_id"
 }
```

尽管ES默认采用随机的文档ID和Hash算法来保证数据均匀分布，但在一些情况下，例如用户自定义文档ID或者指定routing key时，由于hash值可能不够随机，可能会导致数据倾斜的情况。为了避免这种情况，可以考虑在路由key的设计上更加合理，以确保数据在分片中的均衡分布。

前面提到过：**索引设置了主分片数后就不能修改，如果要修改就需要reindex**，很大的原因就是因为路由算法不支持。
![路由计算.png](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/elasticsearch/202311042138279.png)

1. 当我们添加ID=5的文档时，使用公式算出来这个文档应该落在分片1。
2. 修改分片数为5并不进行数据迁移。
3. 查找ID=5的文档，发现使用公式计算出的结果为0，实际上ID=5的文档却是在分片1。

有了路由算法之后，ES可以很容易的计算出每个文档在哪个节点和分片上了。

---

## 2.文档搜索原理
Elasticsearch的搜索过程可以分为两个关键阶段，每个阶段都具有特定的功能，以确保高效的数据检索：

1. **Query阶段**：其主要目的是从协调节点（coordinating node）获取文档的排序值和文档ID。在这个阶段，协调节点会根据查询条件和排序规则，计算每个文档的排序值，并确定要获取的文档ID列表。这个阶段并不涉及获取实际的文档数据，而只关注文档的排序信息和标识。

2. **Fetch阶段**：在Query阶段确定了需要获取的文档ID列表后，就会进入Fetch阶段。在这个阶段，协调节点通过类似于"multi get"的方式，从相应的分片上获取实际的文档数据。每个分片会提供它们所包含的文档数据，以满足查询的需求。

通过将搜索过程分为这两个阶段，Elasticsearch能够更有效地处理大规模数据的搜索请求。在Query阶段，协调节点根据查询条件进行计算和排序，从而快速确定需要获取的文档ID。然后，在Fetch阶段，只有必要的文档数据被获取，减少了数据传输和处理的开销，从而提高了搜索的效率。

---

### 2.1 Query 阶段
在Query阶段，协调节点会按照搜索条件逐个遍历每个分片（可以是主分片或副本分片之一，但不会同时遍历一个分片的主分片和副本分片），以检索满足条件的前N条数据的ID和排序值。随后，在协调节点内对所有分片返回的数据进行整合和排序，以获取最终的前N条数据的ID。
![ES数据搜索流程-Query.png](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/elasticsearch/202311042138540.png)

---

### 2.2 Fetch 阶段
在Fetch阶段，协调节点将根据Query阶段生成的全局排序列表，确定需要返回的文档ID集合。接着，利用路由算法计算出每个文档所属的分片，并以多点获取（multi get）的方式，从相应的分片中检索文档数据。
![数据搜索流程-Fetch阶段.png](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/elasticsearch/202311042138776.png)
**在ES 7.x版本中，处理读取请求时，协调节点会通过逐个轮询所有的副本分片，以实现负载均衡的效果。然而，在进行文档检索时，尽管某个文档已经在主分片上完成了索引，但可能还未来得及复制到相应的副本分片。因此，在这种情况下，副本分片可能会报告该文档不存在，但主分片却有可能成功地返回该文档。只有当索引请求成功地返回给用户后，该文档才会在主分片和副本分片上变得完全可用。这个过程可以看作是一种确保数据一致性的机制，尽管在某些情况下可能会导致一时的不一致性。**

---

### 2.3 再谈深度分页
在前面我们已经介绍了深度分页的概念，现在我们来简要梳理一下搜索流程。然而，采用 Query + Fetch 方式在某些情况下会带来一些问题：

1. 需要从每个分片中检索 `from + size` 个文档。
2. 协调节点需要处理 `shard_amount * (from + size) `个文档。

如果` from + size` 的值较大，协调节点需要处理的数据量也会相应增加，这便是深度分页问题的根源。前面我们已经提到，为了应对深度分页的问题，ES 提供了三种不同的 API。

那么为何使用 `from + size` 而不是 from 到 size 呢？这是因为可能存在这样的情况：分片 1 的第 1 条数据可能比分片 2 的第 1000 到 1100 条数据更大，因此采用 `from + size` 的方式能更精确地获取所需的数据。

---

### 2.4 相关性打分偏离
在 Query 阶段，我们能够观察到文档的评分是在各自的分片上进行计算的。由于每个分片都代表着 Lucene 索引的一部分，因此分片内部独立地进行相关性算分。这种分散的评分计算可能导致文档的打分产生偏差。那么，我们应该如何解决这个问题呢？

1. 对于索引数据较小的情况，可以考虑将主分片数设置为1。
2. 对于大规模的索引数据，我们需要确保数据能够均匀地分布在各个分片之间。
3. 使用 `"DFS Query Then Fetch"` 模式，在请求的 URL 参数中添加：`_search?search_type=dfs_query_then_fetch`。通过这种设定，系统会首先汇总每个分片的词频和文档频率数据到协调节点进行处理，然后再进行相关性评分。需要注意的是，**这种方式会消耗更多的 CPU 和内存资源，因此效率较低**。

相对而言，第二种方法可能是最好的选择。为了确保数据能够均匀地分布在各个分片之间，我们最好不要修改 Elasticsearch 默认的随机文档 ID 和 Hash 算法，也尽量避免显式指定  `_routing key`。这样能够有效减少评分偏差的发生。

---

## 3.文档索引原理
### 3.1 文档写流程
ES文档的分布式存储首先需要确定能够存储文档的主分片，并将相应的数据写入主分片所在的节点。在主分片成功写入数据后，数据会被分发到副分片进行存储。文档的新增、更新、删除等操作都属于写入操作。单个文档的写入操作步骤如下：
![文档写入流程.png](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/elasticsearch/202311042139787.png)

---

### 3.2 文档索引流程
对于文档写入操作的描述，我们从集群的角度进行了说明。然而，在数据到达分片后，还需要进行进一步的处理，包括内容的分词和索引等操作。Elasticsearch中的数据可以大致分为两种类型：

1. **全文本数据**：例如短信内容、文章内容等。这些数据需要经过分词器的处理，将文本内容划分为单词或词条，以便于后续的搜索和查询操作。

2. **精确值数据**：如实体ID、日期等。这类数据不需要进行分词处理，因为它们通常表示唯一的值或确切的范围。

在数据入库阶段，对于精确值数据，不需要进行分词处理，直接存储即可。而对于全文本数据，需要使用适当的分词器对内容进行分词，将文本拆分成有意义的词汇单元，以便于后续的全文搜索和匹配。

在查询阶段，精确值数据的处理相对简单，可以进行精确的等值或不等值比较。而对于全文本查询，情况较为复杂。由于全文本数据已经被分词，查询时无法简单地进行精确的等值匹配。相反，Elasticsearch会根据词汇的相关性进行算分排序，以便返回与查询条件最相关的文档。
![文档索引流程.png](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/elasticsearch/202311042139137.png)

---

### 3.3 数据落盘流程
到目前为止，我们已经初步了解了文档的写入流程。那么，在数据到达分片后，如何确保数据的持久化并提高系统的吞吐量呢？让我们继续深入探讨这些问题。

在Elasticsearch中，数据的持久化是通过将文档写入分片的索引文件中来实现的。索引文件是实际存储数据的文件，它们位于分片所在节点的磁盘上。当文档写入主分片后，主分片会将写入操作复制到副本分片，确保数据的冗余存储。

**为了保证数据的不丢失，Elasticsearch采取了一些重要的策略和机制：**

1. **刷新操作**（Refresh）： 在默认情况下，Elasticsearch使用一种叫做“近实时”（Near Real-Time）的机制。这意味着写入操作不会立即写入磁盘，而是暂存在内存中的缓冲区中。当缓冲区达到一定大小或者一定时间间隔后，会触发刷新操作，将缓冲区中的数据写入磁盘。这个过程是自动进行的，并且可以通过参数进行配置。

2. **事务日志**（Transaction Log）： Elasticsearch使用事务日志（也称为事务日志或WAL）来记录每个写入操作。这个日志记录了写入操作的细节，包括文档的内容和位置。在发生节点崩溃等情况时，系统可以使用事务日志来恢复数据，从而避免数据丢失。

3. **副本复制**： 主分片将写入操作复制到副本分片，确保数据的冗余存储。这意味着即使主分片发生故障，数据仍然可以从副本中恢复。

**为了提高系统的吞吐量，Elasticsearch采用了以下方法：**

1. **批量操作**： 将多个写入操作合并为一个批量操作可以显著提高写入吞吐量。通过使用Bulk API，可以将多个文档一次性提交到索引中，减少了网络开销和请求处理的时间。

2. **并行化**： Elasticsearch是分布式的，可以将写入操作并行分发到不同的分片上，从而充分利用集群中的多个节点和资源，提高写入性能。

3. **硬件优化**： 使用高性能的硬件和适当的硬盘类型（如SSD）可以显著提升数据的写入速度和持久化性能。

通过这些策略和机制，Elasticsearch在保证数据持久化的同时，也能够提高系统的吞吐量，确保写入操作的高效性和可靠性。

Elasticsearch（ES）中的数据落盘过程涵盖了几个关键步骤，其中包括刷新（Refresh）、写入事务日志（Transaction Log）、刷盘（Flush）和合并（Merge）等环节。让我们更详细地了解这些步骤，以便理解数据持久化和系统性能的维护。

---

#### 1）近实时搜索
我们知道倒排索引这种数据结构是不可变的。那么如何在保留不变性的前提下实现倒排索引的更新?

倒排索引的更新机制是通过引入更多的索引段来实现的，而不是通过直接对旧的段进行修改或标记删除。这个机制允许Elasticsearch在保持数据不变性的同时进行更新，同时也是Lucene引擎的核心特性之一。

在文档写入阶段，Elasticsearch（ES）采取了一种智能的写入策略。具体而言，当文档需要写入时，ES首先将这些文档暂时保存在一个称为“Index Buffer”的内存缓冲区中。然后，这些文档会被批量写入到硬盘中，并随后清空Index Buffer。每一批写入硬盘的数据被称为一个“分段”（Segment）。

一般情况下，ES在执行写入操作时会调用操作系统的write系统调用。这个write函数实际上只是将数据暂时写入操作系统的缓存中。这种设计在某种程度上提高了写入效率，因为操作系统缓存的写入速度要远远快于直接写入硬盘。然而，需要注意的是，如果在数据写入操作系统缓存后发生断电或其他意外情况，这部分数据可能会丢失，因为它们还没有真正写入到持久存储介质中。

不过，由于ES的智能写入策略，先将文档写入Index Buffer的举措极大地提高了系统的写入效率。一旦操作系统的write操作完成，数据就会存在于操作系统的缓存中。这意味着，即使文档尚未完全写入硬盘，它们仍然可以被快速地检索和查询。

**Index Buffer中的内容写入到文件以生成分段（Segment）的过程被称为"刷新"（Refresh）**。值得注意的是，Refresh操作并不会调用fsync等刷盘操作。

默认情况下，刷新操作每秒执行一次。可以通过配置`index.refresh_interval`来调整刷新频率。此外，当Index Buffer被写满时，也会触发自动刷新操作。这个Buffer的容量默认为JVM内存的10%。

**通过Refresh机制可以看出Elasticsearch实际上是一个近实时的系统。**换句话说，文档写入成功后，默认情况下，需要过大约一秒钟的时间，才能在查询中看到这些新写入的文档。因此，如果你的业务需求是在写入数据后立即返回到列表页并立即执行数据检索，一定要注意这个时间延迟的问题。

---

#### 2）数据不丢失
默认情况下，当文档写入时，数据并不会立即被持久地写入磁盘，这就可能导致数据丢失的风险。为了避免这种情况，Elasticsearch采取了多重策略以确保数据的可靠性。

在文档写入的过程中，不仅会将数据写入Index Buffer，还会同时写入一个事务日志（Transaction Log）文件。而在7.X版本中，Transaction Log默认是会刷盘的，即写入磁盘。每个分片都拥有自己独立的事务日志。当执行Refresh操作时，系统会清空Index Buffer，但不会清空Transaction Log。这个事务日志的存在，是为了防止数据丢失的关键因素。

在系统重启的情况下，Elasticsearch能够从事务日志中恢复数据。这种机制确保了即使在发生意外宕机或重启的情况下，数据也不会丢失。通过事务日志，系统可以重新构建出之前未来得及刷入磁盘的数据，从而保障了数据的完整性和可靠性。

总的来说，Elasticsearch（ES）在文档写入时首先将数据写入Index Buffer和Transaction Log，这样的设计有着明确的目的和优势。事实上，这种策略结合了内存和持久存储的优势，以确保高效的写入过程和数据的可靠性。

Transaction Log的刷盘策略是默认开启的，这是为了保证数据的持久性。尽管写入Transaction Log需要进行刷盘操作，但因为事务日志的特性，刷盘操作是有序的，也就是顺序写磁盘，从而使得这一步的速度相对较快。事务日志的作用在于记录每次写入操作，以便在系统宕机或重启时能够从日志中恢复数据状态，确保数据不会丢失。尽管刷盘会带来一些性能开销，但它是保障数据完整性的重要步骤。

在写入文档方面，将数据先写入Index Buffer（内存缓冲区）有着显著的性能优势。这个设计的思想是将一批数据暂存于内存中，然后批量写入磁盘，从而实现高效的批量写入操作，远远快于逐条写入磁盘。这也包含了Lucene索引的写入过程，其中包括了数据校验等操作，有助于提高数据的写入效率。另外，通过先写入Index Buffer，可以减少写入失败时的回滚操作，提高了整体的写入稳定性。

---

#### 3）数据持久化
通过执行Flush操作，Elasticsearch（ES）可以将操作系统缓存中的数据写入磁盘。Flush操作在ES中是一个重要的过程，它涉及数据的持久性和内存的释放。具体来说，Flush操作会触发Refresh，将当前Index Buffer中的数据写入操作系统的缓存，并且在调用fsync时将缓存中的数据刷写到磁盘，同时还会清空事务日志（Transaction Log）。因为涉及刷盘操作，Flush过程相对耗时。

Flush操作有两个主要的触发点：

1. **定时触发**： 默认情况下，ES每隔30分钟会自动调用一次Flush操作。这有助于确保数据的持久性，即使在长时间没有写入的情况下也能保证数据得到刷写到磁盘。
2. **Transaction Log容量触发**： ES设置了事务日志的默认容量为512MB，这是通过index.translog.flush_threshold_size进行控制的。当事务日志达到设定的容量时，会触发Flush操作。这是为了避免事务日志过大而引起性能问题，同时也确保事务日志中的数据能够及时刷写到磁盘。

---

#### 4）段合并
在每次执行Refresh操作后，Elasticsearch会生成一个新的分段文件（Segment）。然而，随着分段文件的增多，可能会引发一些问题。这些分段文件不仅会占用文件句柄和内存，还会影响搜索的效率。每次搜索操作都需要检查每个分段文件，然后再合并结果，随着分段文件的增多，搜索速度会逐渐减慢。

为了应对这个问题，需要采取一定的策略，将这些小的分段文件合并为较大的分段文件。段合并的时候会将那些旧的已删除文档从文件系统中清除。被删除的文档(或被更新文档的旧版本)不会被拷贝到新的大段中。启动段合并不需要你做任何事。进行索引和搜索时会自动进行。

1. **刷新（Refresh）操作**：在索引阶段，刷新操作会创建新的段并将这些段打开，以供搜索使用。
2. **合并进程**：Elasticsearch的合并进程会选择一小部分大小相近的段，并在后台将它们合并成更大的段。这一过程不会中断索引和搜索的进行，保证了索引的高效性。
3. **段的删除**：一旦合并完成，旧的段会被删除。同时，新的段被刷新（flush）到磁盘上，生成一个新的提交点。
4. **新的段的刷新和打开**：刷新操作将新的段写入磁盘，同时将包含新段且排除旧且较小的段的提交点进行写入。这个新的段也会被打开，使得它可以被搜索操作使用。

Elasticsearch和Lucene会自动地执行合并（Merge）操作，当然，用户也有能力手动触发这个过程：
```
POST index/_forcemmerge
```

---

### 3.4 文档更新流程
![部分更新文档流程.png](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/elasticsearch/202311042139727.png)
值得特别注意的是，在主分片将更改转发到副本分片时，并不是简单地转发更新请求。相反，主分片会发送包含完整文档的新版本。这是出于保障数据完整性和一致性的考虑，因为仅转发更改请求可能导致错误的更改顺序应用到副本分片，最终破坏文档。

此外，需要注意的是，这些更改会以异步的方式传播到副本分片，因此不能保证它们会按照与发送它们的顺序相同的顺序到达。这也是 Elasticsearch 采取的异步分发机制，以确保高性能和可扩展性的方式。

---

### 3.5 文档批处理流程
mget和bulk API的模式类似于单文档模式，但在一些关键方面有所不同。区别在于，协调节点知道每个文档存在于哪个分片中。它会将整个多文档请求分解为每个分片的多文档请求，然后将这些请求并行地转发到每个参与节点。

一旦协调节点收到来自每个节点的响应，它会将每个节点的响应收集整理成单个响应，并将这个整合后的响应返回给客户端。这个过程确保了多文档请求的正确协调和整合，使得客户端能够得到一个统一和完整的响应。
![MGET处理流程.png](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/elasticsearch/202311042139356.png)
Bulk API允许在单个批量请求中执行多个创建、索引、删除和更新操作。同时可以为docs数组中的每个文档单独设置routing参数。
![Bulk API 处理流程.png](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/elasticsearch/202311042139011.png)
**主分片写副本分片的过程是异步的，那么ES如何确保最终主分片和副本分片的数据一致性？**

1. **版本控制和冲突解决**： Elasticsearch使用每个文档的版本号来确保数据的一致性。当副本分片接收到主分片的数据更新后，它会比较版本号，只有在版本号较新的情况下才会应用更新。如果副本分片上的文档版本较新，则说明在接收更新之前已经有其他更新，这时副本分片会忽略当前更新，以保持较新的版本。

2. **同步和复制**： 副本分片会定期从主分片中获取数据更新。Elasticsearch使用一种叫做“复制”（replication）的机制，通过将主分片上的更新复制到副本分片，从而确保副本分片的数据保持最新。即使写入顺序略有不同，这些复制操作会持续进行，最终使得副本分片与主分片数据保持一致。

3. **分片恢复**： 如果某个副本分片因为任何原因变得过时或丢失，Elasticsearch会自动触发分片恢复过程。这意味着系统会从主分片重新复制数据到副本分片，确保副本的数据重新与主分片同步。

4. **自动重新平衡**： Elasticsearch具有自动平衡数据分布的功能。如果节点失效或新节点加入集群，系统会重新分配分片，确保每个副本分片都能得到正确的数据，并且主分片和副本分片之间的数据保持一致。

---


