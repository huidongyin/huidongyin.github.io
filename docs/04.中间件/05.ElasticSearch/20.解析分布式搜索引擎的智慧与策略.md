---
title: 解析分布式搜索引擎的智慧与策略
date: 2021年9月11日22:50:43
permalink: /pages/d6d68405-7fb2-32a2-9632-26899b0f803e/
tags:
  - 搜索引擎
  - ElasticSearch
author:
  name: huidong.yin
  link: https://huidongyin.github.io
categories:
  - ElasticSearch
---

在当今信息时代，海量数据的高效存储和快速检索变得日益重要。Elasticsearch（ES），作为一款分布式搜索和分析引擎，正以其卓越的性能和灵活性引领着数据处理的前沿。尤其是在ES 7.x版本中，一系列创新性的机制和功能为数据管理带来了前所未有的便捷和效率。

本文将深入探究Elasticsearch 7.x的关键主题，从集群架构到数据处理，从故障处理到性能优化，为您揭示ES在面对各类挑战时的智慧和策略。我们将探讨Master单点故障、集群脑裂的应对策略，探讨CAP原则在分布式环境下的取舍，探寻水平扩容的实现方式，以及分片故障转移的机制。通过对这些关键主题的深入分析，您将更好地理解ES 7.x版本在处理海量数据时所展现的强大能力和高效性能。

---

## 1.Master单点故障
在Elasticsearch中，运行中的一个实例被称为一个节点，而集群则由一个或多个具有相同cluster.name配置的节点组成，这些节点共同分担着数据存储和负载处理的任务。当有新节点加入集群或从集群中移除节点时，数据会被重新平均分布以维持平衡。

> 仅当位于同一台机器上的节点才会自动组成集群。然而，在不同机器上启动节点时，为了使它们加入同一集群，需要配置一个可连接到的单播主机列表。采用单播发现的配置，是为了防止节点无意中加入到不同的集群中。


当一个节点被选举为主节点时，它将负责管理整个集群的变更，如索引的增删、节点的加减等。主节点并不处理文档级别的变更和搜索操作，因此即使集群只有一个主节点，它也不会成为性能瓶颈，即使在流量增加的情况下。任何节点都有可能成为主节点。

作为用户，我们可以将请求发送到集群中的任何节点，包括主节点。每个节点都了解任意文档的位置，可以直接将请求转发到存储所需文档的节点。无论请求发送到哪个节点，它都能够从各个包含所需文档的节点中收集数据，并将最终结果返回给客户端。Elasticsearch在背后透明地处理了所有这些管理任务。

Elasticsearch的主从架构赋予了其系统设计更大的简明性，关键的操作都由主节点来执行，如管理集群的元数据等任务。然而，主节点的独特地位也带来了一些潜在挑战。主节点存在单点故障的风险，尤其在集群规模不断扩大的情况下，这一问题变得尤为突出。

为了解决主节点单点故障的问题，通常会将多个节点配置为主节点候选者。这些候选者在主节点宕机时有能力升级为新的主节点，从而有效地应对单点故障的风险。

然而，当主节点发生故障时，候选者之间会竞争成为新的主节点，有时候可能会出现两个主节点同时存在的情况，这种情况被称为“脑裂”。这个问题需要特别关注，因为它可能导致集群的不一致性。

---

## 2.集群脑裂
集群中始终只存在一个主节点，但如果因为网络问题而出现两个主节点，可能导致元数据不一致。一旦网络问题解决，出现疑惑：应以哪个主节点为准？

在主从模式集群中，由于各种原因（如网络分区、高负载导致假死），可能出现两个活跃的主节点，这被称为“脑裂”。脑裂的危害极大，会危及集群数据的最终一致性。

> 一些可能导致脑裂的原因：
> - 网络问题:集群间的网络延迟导致一些节点访问不到 master,认为 master 挂掉了从而选举出新的master,并对 master 上的分片和副本标红,分配新的主分片
> - 节点负载:主节点的角色既为 master 又为 data,访问量较大时可能会导致 ES 停止响应造成大面积延迟,此时其他节点得不到主节点的响应认为主节点挂掉了,会重新选取主节点。
> - 内存回收:data 节点上的 ES 进程占用的内存较大,引发 JVM 的大规模内存回收,造成 ES 进程失去响应。


为防止脑裂，Elasticsearch采用Quorum机制。在选主时，需要超过半数的Master候选节点的参与。例如，若有5个Master候选节点，则需要（5/2）+1=3个节点参与选主才能成功。

在版本7.0之前，我们需手动设置"Quorum"，在elasticsearch.yml文件中配置：

```bash
discovery.zen.minimum_master_nodes = 3 // 上述计算得出的Quorum值
```

然而，手动设置容易出错，特别是在扩容或修改配置时。因此，7.0版本后已移除此配置，Elasticsearch会自动计算并使用合适的"Quorum"值。

此外，**对节点进行角色分离**： 将主节点（Master）与数据节点（Data）的角色分离开来，不将两者合并在同一节点上可以减少节点的负载，降低出现大规模内存回收导致节点失去响应的风险。这有助于保持主节点的稳定性，从而减少集群脑裂的潜在问题。

---

## 3.CAP要选谁？
水平扩容使得Elasticsearch集群能够轻松处理海量数据，同时提升了系统的可用性和数据可靠性。然而，在分布式系统中，必须遵循CAP定理，那么在CAP的三个原则中，Elasticsearch是如何做出选择的呢？

![image.png](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/elasticsearch/202311042146197.png)
CAP定理涵盖了分布式数据存储系统的一致性（Consistency）、可用性（Availability）和分区容错性（Partition Tolerance）中的两个。

1. 一致性：数据一致性，即客户端请求成功后，所有节点上的数据都是一致的。
   1. 强一致性：更新后的数据可以立刻在后续的访问中读到。
   2. 弱一致性：数据更新后，后续对该数据的读取可能得到旧值也可能得到更新后的值。
   3. 最终一致性：一段时间后各个节点的数据会达到一致状态，最终所有请求都会读到更新后的值。
> CAP中的一致性指的是强一致性。

2. 可用性：服务一直可用，系统读写可以再正常的响应时间内返回。
3. 分区容错性：在系统遇到网络故障或者部分节点故障的时候，系统依然可以对外服务。

作为分布式数据存储系统，Elasticsearch必须遵循CAP原则。然而，由于现实中网络环境不稳定，常常会进行多网络分区的部署，因此对网络分区的容忍性（P）是必须要保证的。在剩下的可用性和一致性中，Elasticsearch更倾向于选择保证可用性。
![ES数据写流程.png](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/elasticsearch/202311042146467.png)
数据写入时，会首先写入主分片进行缓存和日志记录，然后再将数据同步到副本分片。尽管主分片上的写入成功后并不能立即查询到，它会默认每秒将缓存数据刷新到磁盘，以便进行检索。因此，Elasticsearch实际上采用的是最终一致性。默认情况下，只需主分片写入成功，不一定需要多个副本写入成功。Elasticsearch定位于准实时系统，注重可用性，而并非必须保证强一致性。

---

## 4.水平扩容
随着系统数据量的增长，可能就会需要对集群进行扩容。当为系统增加数据节点的时候，索引的分片会自动均匀的分配到各个数据节点，从而达到负载均衡的目的。
![集群扩容过程.png](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/elasticsearch/202311042146779.png)
当node-3被添加后，node-1和node-2上各有一个节点迁移到了新的node-3节点，这样每一个节点都拥有两个分片，而不是之前的3个。这表示每个节点的硬件资源（CPU，RAM，IO）将被更少的分片所享受，每个分片的性能将会得到提升。

分片是一个完整搜索引擎的核心功能单元，能够充分利用节点上的全部资源。例如，一个具有6个分片（3个主分片和3个副本分片）的索引，可以将最大扩容至6个节点。每个节点分别负责一个分片，而每个分片都能充分借助其所在节点的资源。

**然而，如果我们需要超过6个节点的扩容呢？**

主分片的数量在索引创建时就被确定，实际上，这个数目决定了索引能够容纳的最大数据量（实际大小由数据、硬件和使用场景决定）。然而，读操作（如搜索和数据返回）可以由主分片或副本分片同时处理。因此，拥有更多副本分片将导致更高的吞吐量。

在运行中的集群上，我们可以动态调整副本分片的数量。我们可以根据需求扩展集群，将副本数从默认的1增加到2，例如：
```json
{
  "number_of_replicas" : 2
}
```

此时，该索引将拥有9个分片，其中三个是主分片，六个是副本分片。这意味着我们可以将集群扩展至9个节点，每个节点负责一个分片。相较于初始的三个节点配置，集群的搜索性能将提升三倍。

当然，在相同节点数量的情况下，增加更多副本分片并不能提升性能，因为每个分片可使用的节点资源减少。此时，需要增加更多的硬件资源（即垂直扩容）来提高吞吐量。不过，更多的副本分片会增加数据冗余量。以上述节点配置为例，即便失去两个节点，仍能保障数据不丢失。

**假设集群有7个数据节点，但是索引设置了3个主分片，分片会如何分配？**

在 Elasticsearch 中，分片的分配是基于主分片和副本分片的设置以及集群的节点数量。7个数据节点，而索引设置了3个主分片。分片的分配将如下进行：

1. 每个主分片会分配到一个不同的数据节点上，以确保数据的均匀分布和高可用性。因此，7个数据节点将承载3个主分片。

2. 由于每个主分片需要有对应的副本分片来提供冗余和高可用性，对于每个主分片，Elasticsearch 默认会创建一个副本分片。这意味着每个主分片都会有一个副本分片，每个分片都需要一个独立的数据节点来承载。

综上所述，在这种情况下，根据3个主分片和7个数据节点，分片的分配将如下进行：

- 主分片1: 数据节点1
- 主分片2: 数据节点2
- 主分片3: 数据节点3

每个主分片都会有一个副本分片，因此副本分片的分配将与主分片相同：

- 副本分片1: 数据节点4
- 副本分片2: 数据节点5
- 副本分片3: 数据节点6

> 分片的分配还可能受到集群的配置、节点属性、数据量和硬件资源等因素的影响。这只是一个简化的例子，实际情况可能会更加复杂。


---

**假设集群有3个数据节点，但是索引设置了7个主分片，会如何分配?**

1.  由于主分片的数量（7个）超过了数据节点的数量（3个），不同于上一个例子，分配将会存在一些限制和情况。 
2.  Elasticsearch 会尝试将主分片均匀地分配到可用的数据节点上。然而，由于主分片的数量多于数据节点，部分主分片将无法分配到节点上。Elasticsearch 会将这些未分配的主分片设置为未分配状态。 
3.  同样，每个主分片需要有对应的副本分片来提供冗余和高可用性。但是，由于主分片数量已经超过了数据节点数量，部分副本分片也将无法分配到节点上。 

综上所述，在这种情况下，根据7个主分片和3个数据节点，分片的分配将如下进行：

已分配的主分片：

- 主分片1: 数据节点1
- 主分片2: 数据节点2
- 主分片3: 数据节点3
- ...

未分配的主分片：

- 主分片4: 未分配
- 主分片5: 未分配
- 主分片6: 未分配
- 主分片7: 未分配

由于副本分片需要与主分片一一对应，因此未分配的主分片也会导致副本分片的未分配：

未分配的副本分片：

- 副本分片1: 未分配
- 副本分片2: 未分配
- 副本分片3: 未分配
- ...

> 这种情况下的分片分配并不是理想的情况，可能会影响集群的性能和可用性。因此，在设置索引主分片数量时，应该根据集群的实际情况和需求进行权衡和规划。


---

**写入的数据可能会写入到未分配的主分片么？**

在 Elasticsearch 中，未分配的主分片是不会参与数据的写入的。写入操作只会涉及到已经成功分配的主分片。未分配的主分片仅在集群中的重新分配过程中才会被重新分配到数据节点上。

当写入数据到索引时，Elasticsearch 会根据数据的路由规则将数据写入对应的主分片和其副本分片。因此，在索引设置了7个主分片，但只有3个数据节点的情况下，只有部分主分片能够被成功分配到数据节点上，而其他未分配的主分片是不会接受写入操作的。

为了确保数据的高可用性和冗余性，Elasticsearch 会将每个主分片的副本分片分配到不同的数据节点上。这样，在节点故障或其他情况下，数据仍然可以从副本分片中恢复。

总之，未分配的主分片不会参与写入操作，而是会在集群进行重新分配时被重新分配到数据节点上，以确保索引的数据均匀分布和高可用性。

---

## 5.分片故障转移
ES通过副本机制来确保数据的可靠性。当一个数据节点出现故障或下线时，系统会自动进行主分片的故障转移，以防止数据丢失。

在集群扩容过程中，我们添加新的数据节点。而**分片故障转移是指当一个数据节点下线后，ES会自动将下线节点上的主分片重新分配到其他节点，并生成相应的副本分片**。主分片的重新分配实际上就是将具备条件的副本分片提升为主分片的过程。通过这种机制，ES保证了数据的高可用性和持久性。
![分片故障转移.png](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/elasticsearch/202311042146276.png)
**副本分片升级为主分片的详细过程如下：**

1. 主分片不可用检测： 当一个主分片变得不可用，通常是由于其所在的数据节点出现故障、网络问题等原因，Elasticsearch会检测到主分片的不可用状态。
2. 副本分片选举： 一旦主分片被标记为不可用，Elasticsearch会从已有的副本分片中选择一个作为新的主分片。这个选举过程通常依赖于分片的复制状态、同步进度以及副本分片所在节点的可用性。
3. 新主分片初始化： 选举出的新主分片会被初始化，这包括从剩余的可用分片中复制数据、索引文档等操作，以使其达到与之前主分片相同的状态。
4. 数据同步： 一旦新的主分片初始化完成，副本分片会开始与新主分片进行数据同步，确保数据的一致性。
5. 更新集群状态： 一旦新主分片和副本分片完成数据同步，Elasticsearch会更新集群状态，将新主分片标记为活跃状态。

副本分片升级为主分片的过程保证了数据的高可用性和可靠性。通过及时选举并初始化新的主分片，Elasticsearch确保了在主分片故障时能够快速恢复并继续提供稳定的查询和写入服务。这也是分布式系统中保障数据一致性和可用性的重要机制之一。这个过程通常是相对较快的，但具体的速度取决于多个因素，包括集群的规模、硬件性能、网络延迟等等。

在正常的情况下，副本分片已经保持与主分片的数据同步，因此升级过程中的数据传输量相对较小，可以在较短的时间内完成。Elasticsearch的设计和优化都旨在尽可能地减少数据传输的开销，从而提高数据同步的效率。

另外，副本分片升级为主分片的速度还取决于集群的负载和资源利用情况。如果集群的负载较高，可能会影响数据同步的速度。同时，硬件性能也会对速度产生影响，性能更好的硬件能够更快地完成数据传输和处理操作。

需要注意的是，副本分片升级为主分片是为了保障数据的可用性和一致性，在此过程中可能会对集群的性能产生一些影响。因此，在副本分片升级为主分片期间，集群的性能可能会有所下降，但通常情况下这个过程会尽量保持在较短的时间内完成，以减少对正常查询和写入操作的影响。

---





