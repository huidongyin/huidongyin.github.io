---
title: 生产者-客户端开发
date: 2023-01-01 00:00:00
tags:
  - Kafka
  - 消息队列
categories:
  - Kafka
description: 生产者-客户端开发
toc_number: false
author:
  name: huidong.yin
  link: https://huidongyin.github.io
permalink: /pages/00e44d72-3af4-33ec-95aa-573c4e2603b1/
---

从下面的代码可以看到，将消息发往kafka主要经历四个步骤。

1. 配置生产者客户端参数以及创建对应的生产者实例。
2. 构建要发送的消息。
3. 发送消息
4. 关闭生产者实例。

```bash
public static void produce() {
    Properties props = new Properties();
    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);
    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());

    KafkaProducer<String, String> producer = new KafkaProducer<>(props);

    ProducerRecord<String, String> record = new ProducerRecord<>(topicName, "key", "value");

    for (int i = 0; i < 100; i++) {
        producer.send(record, (metadata, exception) -> {
            if (exception == null) {
                System.out.println("Message sent to partition " + metadata.partition() + " with offset " + metadata.offset());
            } else {
                exception.printStackTrace();
            }
        });
    }

    producer.close();
}
```

> `ProducerConfig` 是Kafka中提供的常量类。

对于要发送的消息，我们需要构建成`ProducerRecord`对象。但是它并不是单纯意义上的消息，它的本身包含多个属性，原本需要发送的消息仅仅是消息体的其中一个属性（value）。

```bash
public class ProducerRecord<K, V> {

    private final String topic; //消息将要发送到的主题
    private final Integer partition; //消息将要发送到的分区
    private final Headers headers; //消息的header，可以为空
    private final K key; //消息的key
    private final V value; //真正要发送的消息
    private final Long timestamp; //时间戳
}
```

- `topic` 和 `partition` 表示消息的目标主题和分区。
- `header`字段是消息头部。
- `key` 用来指定消息的键，可以用来计算分区号让消息发送特定的分区。同一个key的消息会被发送到同一个分区中，另外有key的消息还支持消息压缩。
- `value` 表示消息体，一般不为空。
- `timestamp` 是指消息的时间戳，它有创建时间和追加到日志的时间这两种类型。

---

## 1.必填参数

- `bootstrap.servers`
- `key.serializer`
- `value.serializer`

对于这三个参数前面已经介绍过，再此不再赘述，对于开发人员来讲，开发的时候直接拼配置字符串不好拼，可以直接使用kafka客户端提供的 `ProducerConfig`类。另外补充介绍一个参数`client.id`，生产者对应的客户端ID，默认为空，如果客户端没有设置，`KafkaProducer`会自动生成一个。

**`KafkaProducer`是线程安全的，可以在一个线程中共享单个`KafkaProducer`实例。**

---

## 2.消息发送

创建完生产者实例，构建完消息以后，就是真正的发送消息。发送消息有三种模式：单向发送，同步发送和异步发送。

**单向发送**：大多数情况下，单向发送没什么问题，不过在发生不可重试异常的时候，会造成消息丢失，这种方式性能最高，但是可靠性最差。

**同步发送**：同步发送的性能最差，可靠性相对最高，需要阻塞等待一条消息发送完以后才能发送下一条。

```bash
Future<RecordMetadata> future = producer.send(record);
RecordMetadata recordMetadata = future.get();
```

实际上send方法本身就是异步的，send方法返回的Future对象可以使调用方稍后获得调用的结果`RecordMetadata`对象。

`RecordMetadata`对象包含消息的一些元数据，类似 `topic` `partition` `offset` 等等。`Future`表示一个任务的生命周期，并提供了相应的方法来判断任务是否已经完成或取消，以及获取任务的结果和取消任务等等。

`KafkaProducer`中一般会发生两种类型的异常：可重试异常和不可重试异常。常见的可重试异常：NetworkException LeaderNotAvailableException UnknownTopicOrPartitionExceptionNotEnoughReplicasException等；对于不可重试异常，例如：RecordTooLargeException异常，表示发送的消息过大，`KafkaProducer`对此不会进行任何重试，直接抛出异常。

对于可以重试的异常，可以通过 `retries` 参数控制重试次数，并且只要在规定的重试次数内自行恢复了，就不会抛出异常。 `retries`参数默认值等于0。配置方式如下：

```bash
props.put(ProducerConfig.RETRIES_CONFIG, 3);
```

如果发生异常以后重试了3次都没有恢复，那么仍然会抛出异常。

**异步发送**：一般是在send方法里面指定一个`Callback`回调函数，Kafka再返回响应时调用该函数来实现异步的发送确认。

> send方法本身就是异步的，为什么还要额外使用Callback做异步？`Future` 里面的get方法在何时调用，以及怎么调用都是需要面对的问题，消息不停的发送，那么消息对应的 `Future`对象的处理难免会引起代码处理逻辑的混乱。使用 `Callback` 的方式非常简单明了，Kafka有响应就会回调，要么成功，要么异常。

```bash
producer.send(record, (metadata, exception) -> {
    if (exception !=null){
        System.out.println("发生异常了！");
        return;
    }
    System.out.println("发送成功！");
  
});
```

lambda表达式的两个参数是互斥的，消息发送成功，metadata 一定不为空，但是exception一定是空，反之发送失败，metadata一定是空，但是exception不为空。另外对于同一个分区而言，如果消息A在消息B之前发送，那么`KafkaProducer`就可以保证对应的A的回调在B的回调之前调用，也就是说，回调函数的调用也可以保证分区有序。

一般在项目关闭时，会释放掉项目中的所有资源，在释放生产者实例对象时，通过调用`KafkaProducer`的close方法来回收资源。close方法会阻塞等待之前所有的发送请求完成后在关闭`KafkaProducer`，如果调用的是带超时时间的close方法，那么只会在等待timeout时间内来完成所有尚未完成的请求处理，然后强行退出。

---

## 3.序列化

生产者需要使用序列器将对象转换成字节数组才能通过网络发送到Kafka。而在另一端，消费者需要使用反序列化器把从Kafka中收到的字节数组转换成对应的对象。

Kafka默认提供了几种序列化器，如下，他们都实现了`Serializer`接口，此接口有四个方法。
![](https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311041748298.png)

```java
public interface Serializer<T> extends Closeable {

    default void configure(Map<String, ?> configs, boolean isKey) {
        // intentionally left blank
    }

    byte[] serialize(String topic, T data);

    default byte[] serialize(String topic, Headers headers, T data) {
        return serialize(topic, data);
    }

    @Override
    default void close() {
        // intentionally left blank
    }
}
```

`configure`方法用来配置当前类，`serialize`方法用来执行序列化操作，`close`方法用来关闭当前序列化容器。

**生产者使用的序列化器和消费者使用的反序列化器是需要一一对应的。**

下面以StringSerializer类来分析序列化器。

首先是configure方法，这个方法是在创建KafkaProducer实例的时候调用的，主要用来确定编码类型，不过一般客户端对于`key.serializer.encoding` ，`value.serializer.encoding`和 `serializer.encoding` 这几个参数不会配置而已。一般情况下，默认的encoding的值就是 UTF-8。

```java
private String encoding = "UTF8";

@Override
public void configure(Map<String, ?> configs, boolean isKey) {
    String propertyName = isKey ? "key.serializer.encoding" : "value.serializer.encoding";
    Object encodingValue = configs.get(propertyName);
    if (encodingValue == null)
        encodingValue = configs.get("serializer.encoding");
    if (encodingValue instanceof String)
        encoding = (String) encodingValue;
}
```

serialize方法比较直观，就是把String类型转换为字节数组类型。

```java
@Override
public byte[] serialize(String topic, String data) {
    try {
        if (data == null)
            return null;
        else
            return data.getBytes(encoding);
    } catch (UnsupportedEncodingException e) {
        throw new SerializationException("Error when serializing string to byte[] due to unsupported encoding " + encoding);
    }
}
```

如果Kafka客户端提供的几种序列化器都无法满足要求，则可以使用如Avro，JSON，Thrift，ProtoBuf等通用的序列化工具来实现，或者自定义序列化器。

使用自定义的序列化器主要分为两步。

1. 实现自定义的序列化器。
2. 将自定义的序列化器指定给Kafka。

例如我们实现一个仅有两个属性的User类的序列化器。

```java
public class UserSerializer implements Serializer<UserSerializer.User> {

    private static final String encoding = "UTF-8";
  
    @Override
    public byte[] serialize(String topic, User data) {
        if (data == null) {
            return null;
        }
        try {
            byte[] id, name;
            if (data.getId() != null) {
                id = data.getId().getBytes(encoding);
            }else{
                id = new byte[0];
            }
            if (data.getName()!=null){
                name =data.getName().getBytes(encoding);
            }else{
                name = new byte[0];
            }
            ByteBuffer buffer = ByteBuffer.allocate(4 + 4 + id.length + name.length);
            buffer.putInt(id.length);
            buffer.put(id);
            buffer.putInt(name.length);
            buffer.put(name);
            return buffer.array();
        } catch (UnsupportedEncodingException e) {
            e.printStackTrace();
        }
        return new byte[0];
    }
  
  
    @Data
    public static class User implements Serializable {
        private String id;
  
        private String name;
    }
}
```

如何在Kafka指定自定义的序列化器？只需要将`value.serializer`参数设置为自定义序列化器的全限定类名即可。

```java
props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, UserSerializer.class.getName());
```

> 如果仅仅指定消息的默认序列化器，key对应的序列化器还是默认的`StringSerializer`。

---

## 4.分区器

消息在通过send方法发送到broker的过程中，可能需要经过拦截器，序列化器和分区器的一系列作用。消息经过序列化之后就需要确定它发往的分区，如果消息`ProduceRecord`中指定了`partition`字段，那么就不需要分区器的作用，因为`partition`代表的就是所要发往的分区号。总的来说，**分区器的作用就是为消息分配分区**。

kafka提供的默认分区器是`DefaultPartitioner`，它实现了Partitioner接口，这个接口中定义了三个方法。

```java
public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster);


public void close();


default public void onNewBatch(String topic, Cluster cluster, int prevPartition) {
}
```

partition方法用来计算分区号，返回值为int类型，partition方法中的参数分别表示主题，键，序列化后的键，值，序列化后的值，以及集群的元数据信息。

close方法用于在关闭分区器的时候实现一些资源的回收。

在默认的分区分配方法中，如果key不为null，那么默认的分区器会对key进行Hash，最终根据得到的hash值来计算分区号，拥有相同key的消息会被写入同一个分区。如果key为null，那么消息会以特定轮询的方法发往主题内的每个可用分区。

**在不改变主题分区数量的情况下，key与分区之间的映射可以保持不变，不过一旦主题中增加了分区，那么就难以保证key与分区之间的映射关系了。**

除了使用kafka默认提供的分区器进行分区分配，还可以使用自定义的分区器，只需要同`DefaultPartitioner`一样实现`Partitioner`接口即可。

实现自定义的分区器以后，需要通过配置参数 `partitioner.class` 来显式指定这个分区器。

```java
props.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, DefaultPartitioner.class.getName());
```

---

## 5.拦截器

kafka一共有两种拦截器：生产者拦截器和消费者拦截器。生产者拦截器可以用来在消息发送前做一些准备工作，比如按照某个规则过滤不符合要求的消息，修改消息的内容等等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。

生产者拦截器的使用需要实现ProducerInterceptor接口，该接口包含三个方法。

```java
public interface ProducerInterceptor<K, V> extends Configurable {

    public ProducerRecord<K, V> onSend(ProducerRecord<K, V> record);

    public void onAcknowledgement(RecordMetadata metadata, Exception exception);

    public void close();
}
```

KafkaProducer在将消息序列化和计算分区之前会调用生产者拦截器的onSend方法来对消息进行相应的定制化操作。

KafkaProducer会在消息被应答之前或者消息发送失败时调用生产者拦截器的onAcknowledgement方法，优先于用于设置的Callback之前执行。这个方法运行在生产者的IO线程中，所以逻辑越简单越好或者使用异步的处理方式。close方法用于在关闭拦截器时执行一些资源的清理操作，这三个方法抛出的异常会被捕获并记录到日志中，但并不会向上传递。

实现自定义的拦截器之后，需要在KafkaProducer的配置参数 `interceptor.classes` 中指定这个拦截器，此参数默认值时空字符串。

```java
props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, "");
```

KafkaProducer可以指定多个拦截器形成拦截器链。拦截器链会按照`interceptor.classes`参数配置的拦截器顺序一一执行，配置的时候，多个拦截器之间使用逗号分隔。

如果拦截器链中的某个拦截器的执行需要依赖于前一个拦截器的输出，那么就有可能产生副作用。**如果某个拦截器链执行失败，那么下一个拦截器会接着上一个执行成功的拦截器继续执行。**

---

