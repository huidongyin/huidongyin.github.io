---
title: Zookeeper入门
date: 2023-01-01 00:00:00
tags: 
  - Zookeeper
  - 中间件
categories: 
  - Zookeeper
description: Zookeeper入门
toc_number: false
author:
  name: huidong.yin
  link: https://huidongyin.github.io
permalink: /pages/zookeeper/
---

## 1.为什么要有Zookeeper

对于单体应用，一次用户下单的流程可能如下：`用户->服务->MySQL`。

对于微服务架构，一次用户下单的流程可能如下：`用户->服务1->服务2->MySQL`。

服务与服务之间一定不是相互隔离的，而是必须要相互联系进行数据通信才能实现完整功能。服务之间远程通信可以使用RPC或者HTTP。有了远程通信以后，我们势必会考虑几个问题：

1. 目标服务扩容缩容，客户端会带来一些变化。
2. 客户端对于目标服务如何负载均衡。
3. 客户端如何维护目标服务的地址信息。
4. 服务端的服务状态发现，如何让客户端尽心感知。

此时最好的办法就是有一个服务专门用来服务的注册和发现，也就是注册中心，在微服务架构中，他起到了非常大的做用。Dubbo体系中的Zookeeper，SpringCloud体系中的Eureka，Nacos都是注册中心的具体实现。

---

## 2.什么是Zookeeper

Apache ZooKeeper 是一个高可靠的分布式协调中间件。它是 Google Chubby 的一个开源实现，那么它主要是解决什么问题的呢？那就得先了解 Google Chubby。Google Chubby 是谷歌的一个用来解决分布式一致性问题的组件，同时，也是粗粒度的分布式锁服务。

### 2.1 分布式一致性问题

什么是分布式一致性问题呢？简单来说，就是在一个分布式系统中，有多个节点，每个节点都会提出一个请求，但是在所有节点中只能确定一个请求被通过。而这个通过是需要所有节点达成一致的结果，所以所谓的一致性就是在提出的所有请求中能够选出最终一个确定请求。并且这个请求选出来以后，所有的节点都要知道。

**分布式一致性的本质，就是在分布式系统中，多个节点就某一个提议如何达成一致。**

在 Google 有一个 GFS(google file system)，他们有一个需求就是要从多个 gfs server 中选出一个 master server。这个就是典型的一致性问题，5 个分布在不同节点的 server，需要确定一个 master server，而他们要达成的一致性目标是：确定某一个节点为 master，并且所有节点要同意。而 GFS 就是使用 chubby 来解决这个问题的。

实现原理是：所有的 server 通过 Chubby 提供的通信协议到 Chubby server 上创建同一个文件，当然，最终只有一个 server 能够获准创建这个文件，这个 server 就成为了 master，它会在这个文件中写入自己 的地址，这样其它的 server 通过读取这个文件就能知道被选出的master 的地址。

---

### 2.2 分布式锁服务

从另外一个层面来看，Chubby 提供了一种粗粒度的分布式锁服务，chubby 是通过创建文件的形式来提供锁的功能，server 向 chubby 中创建文件其实就表示加锁操作，创建文件成功表示抢占到了锁。

由于 Chubby 没有开源，所以雅虎公司基于 chubby 的思想，开发了一个类似的分布式协调组件 Zookeeper，后来捐赠给了 Apache。Zookeeper并不是作为注册中心而设计，它是作为分布式锁的一种设计。而注册中心只是它能够实现的一种功能而已。

---

### 2.3 Zookeeper管理元数据的优势

首先它可以很好地支持集群部署；其次它具有很好的分布式协调能力，假设对 ZooKeeper 中的数据做了变更（比如新增了一台 Kafka 或者挂掉了一个 Kafka 节点），这时候 ZooKeeper 会**主动通知**其他监听这个数据的客户端，立即告诉其他客户端说这份元数据有变更。

ZooKeeper 的设计十分巧妙，它的主动通知机制采取的是 `Watcher 机制`，至于什么是 Watcher 机制，后面文章会详细地剖析其思想和源码。

知道了 ZooKeeper 的应用场景后，再来想想 **目前哪些流行框架使用了 ZooKeeper** 。

* `Dubbo`：非常流行的 RPC 框架，它就采取了 ZooKeeper 作为注册中心，来管理分布式集群的元数据存储。当然也可以采取 Nacos 作为注册中心。
* `Kafka`：消息中间件，它采取了 ZooKeeper 做分布式集群的元数据存储以及分布式协调功能。
* `HBase`：大数据领域的技术，它也采取了 ZooKeeper 做分布式集群的元数据存储。
* `HDFS`：大数据领域的技术，它采取了 ZooKeeper 做 Master 选举实现 HA 高可用的架构。
* `Canal`：ETL 工具，监控 binlog 做数据同步，它采取了 ZooKeeper 做分布式集群的元数据存储，也用 ZooKeeper 做 Master 选举实现 HA 高可用的架构。
* ……

其实不管是 RPC 也好，消息中间件也罢，它们都需要注册中心，只是技术选型到底该用哪个的事情。比如，Kafka 用了 ZooKeeper，但是 RocketMQ 就用了自研的 Nameserver；再比如，SpringCloud 的 Feign 就采取了 Eureka 和 Nacos。

---

### 2.4 ZooKeeper 和 Eureka/Nacos的区别

区别有很多，这里只介绍一点：ZooKeeper 是 CP，而 Eureka 是 AP，Nacos 既可以是 AP 又可以是 CP。那什么是 CP？什么又是 AP？

ZooKeeper 集群是 Leader 负责写，写成功后是同步到各个 Follower 从节点。那么问题来了，如果这时候 Leader 挂了，Follower 会进行选举，但是选举也需要时间的，选举过程中如果进来了读写请求，那么是无法进行的。所以会有部分流量的丢失，这就是所谓的 `CP 模型`，**用服务的可用性来换取数据的相对强一致性。**

再比如：一个集群 5 个节点，按照过半原则来讲，那么 3 个节点是半数以上，假设集群内挂了 3 台，只保留了 2 台存活节点，那么这时候集群也是无法提供读写请求的，因为不符合过半原则了，这也是 CP 的特征之一。

这样有什么问题？很明显，整个集群的可用性相对较低，因为假设我就 3 个节点，那么挂了 2 个后其实还有 1 个存活，这个存活的节点却不能提供服务。

那 ZooKeeper 的选主机制效率高吗？官方给了个压测的结果，不会超过 200ms。

![s](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/847e6f57078d4a7fade6e088d2d9dc52~tplv-k3u1fbpfcp-jj-mark:1512:0:0:0:q75.awebp)

上面介绍的就是分布式系统 CAP 理论中的 CP 模型，本质就是：**用服务的可用性来换取数据的相对强一致性。**

---

## 3.Zookeeper安装

这里介绍使用Docker安装Zookeeper集群。

首先创建每一个节点对应的容器映射目录 `conf,data,logs`。

接下来在每一个 `data`目录下创建一个 `myid`文件，并指定节点的id（id不能重复）。

![1702364141326](image/1.zookeeper入门/1702364141326.png)

接下来在 `conf`目录下创建 `zoo.cfg`配置文件，加入以下内容：

```plaintext
tickTime=2000
initLimit=5
syncLimit=2
dataDir=/data
dataLogDir=/datalog
autopurge.snapRetainCount=3
autopurge.purgeInterval=0
maxClientCnxns=60
admin.enableServer=true

server.1=zk1:2888:3888;2181
server.2=zk2:2888:3888;2182
server.3=zk3:2888:3888;2183
```

这些配置项在 ZooKeeper 的配置文件 `zoo.cfg` 中定义了 ZooKeeper 的运行参数和集群配置。下面是这些配置项的意义和作用：

- **tickTime**: 这是 ZooKeeper 用于计算时间的基本单位，以毫秒为单位。ZooKeeper 使用 tickTime 来进行心跳、超时等时间相关的操作。在 ZooKeeper 集群中，所有节点必须使用相同的 tickTime 值。
- **initLimit**: 这个配置项规定了 ZooKeeper 集群中的跟随者（follower）和领导者（leader）之间初始连接建立时最大允许的时间间隔数。这个时间间隔以 tickTime 的倍数来表示，用于保证 ZooKeeper 集群中的初始化同步速度。
- **syncLimit**: 这个配置项规定了 ZooKeeper 集群中的跟随者和领导者之间发送消息的最大时间间隔数。和 `initLimit` 类似，也是以 tickTime 的倍数来表示，用于保证 ZooKeeper 集群中的消息同步速度。
- **dataDir**: 这是 ZooKeeper 存储数据快照（snapshot）的目录。ZooKeeper 会在这个目录下存储数据文件。
- **dataLogDir**: 这是 ZooKeeper 存储事务日志文件的目录。ZooKeeper 将在这个目录下存储事务日志。
- **autopurge.snapRetainCount**: 当启用自动清理（autopurge）时，这个配置项指定保留的快照文件数量。默认为 3。
- **autopurge.purgeInterval**: 当启用自动清理时，这个配置项指定自动清理事务日志和快照的时间间隔。设置为 0 表示禁用自动清理。
- **maxClientCnxns**: 这个配置项规定了单个客户端（连接到 ZooKeeper 的应用程序）与 ZooKeeper 服务器的最大连接数。
- **admin.enableServer**: 这个配置项用于启用或禁用四字命令（`mntr`、`conf` 等）。当设置为 true 时，允许使用四字命令。
- **server.X**: 这是配置 ZooKeeper 集群中的每个节点。`X` 是节点的 ID，后面是节点的通信地址和端口号。格式为 `server.X=hostname:peerPort:leaderPort;clientPort`。其中 `peerPort` 是用于节点之间通信的端口，`leaderPort` 是用于选举 Leader 的端口，`clientPort` 是 ZooKeeper 客户端连接的端口。

举例来说，`server.1=zk1:2888:3888;2181` 表示节点1，通信地址为 `zk1`，节点之间通信端口为 `2888`，选举 Leader 的端口为 `3888`，客户端连接端口为 `2181`。

最后在命令行执行docker命令启动三个Zookeeper节点。

```plaintext
docker network create zookeeper-net


docker run -d   -p 2181:2181  --network zookeeper-net  --name zk1  -v /Users/HuidongYin/docker/zk/zk1/data:/data  -v /Users/HuidongYin/docker/zk/zk1/conf:/conf  -v /Users/HuidongYin/docker/zk/zk1/logs:/datalog  zookeeper:3.7.1

docker run -d   -p 2182:2181  --network zookeeper-net  --name zk2  -v /Users/HuidongYin/docker/zk/zk2/data:/data  -v /Users/HuidongYin/docker/zk/zk2/conf:/conf  -v /Users/HuidongYin/docker/zk/zk2/logs:/datalog  zookeeper:3.7.1

docker run -d   -p 2183:2181  --network zookeeper-net  --name zk3  -v /Users/HuidongYin/docker/zk/zk3/data:/data  -v /Users/HuidongYin/docker/zk/zk3/conf:/conf  -v /Users/HuidongYin/docker/zk/zk3/logs:/datalog  zookeeper:3.7.1
```


`docker network create zookeeper-net`命令创建了一个名为 `zookeeper-net` 的 Docker 网络。这个网络用于连接多个容器，使它们可以相互通信。

`docker run -d -p 2181:2181 --network zookeeper-net --name zk1 -v /Users/HuidongYin/docker/zk/zk1/data:/data -v /Users/HuidongYin/docker/zk/zk1/conf:/conf -v /Users/HuidongYin/docker/zk/zk1/logs:/datalog zookeeper:3.7.1`命令启动了一个名为 `zk1` 的 ZooKeeper 容器。解释如下：

- `-d`: 表示在后台运行容器。
- `-p 2181:2181`: 将主机的端口 `2181` 映射到容器的 `2181` 端口，用于 ZooKeeper 的客户端连接。
- `--network zookeeper-net`: 将容器连接到名为 `zookeeper-net` 的 Docker 网络。
- `--name zk1`: 指定容器的名称为 `zk1`。
- `-v /Users/HuidongYin/docker/zk/zk1/data:/data`: 将本地机器上的 `/Users/HuidongYin/docker/zk/zk1/data` 目录挂载到容器中的 `/data` 目录，用于存储 ZooKeeper 数据。
- `-v /Users/HuidongYin/docker/zk/zk1/conf:/conf`: 将本地机器上的 `/Users/HuidongYin/docker/zk/zk1/conf` 目录挂载到容器中的 `/conf` 目录，用于存储 ZooKeeper 的配置文件。
- `-v /Users/HuidongYin/docker/zk/zk1/logs:/datalog`: 将本地机器上的 `/Users/HuidongYin/docker/zk/zk1/logs` 目录挂载到容器中的 `/datalog` 目录，用于存储 ZooKeeper 的日志文件。
- `zookeeper:3.7.1`: 使用 Docker Hub 上的 `zookeeper:3.7.1` 镜像来创建容器。

其余两个命令和上面的命令类似，区别在于将分别创建名为 `zk2` 和 `zk3` 的 ZooKeeper 容器，并使用不同的本地目录作为数据目录、配置文件目录和日志目录，同时映射不同的主机端口到容器的 `2181` 端口。这样可以在同一网络中启动多个 ZooKeeper 节点，组成一个 ZooKeeper 集群。

如何验证Zookeeper集群创建成功呢？

使用 `docker exec 容器id /bin/bash`命令进入docker容器，接下来执行下面的命令：

```plaintext
# cd bin
# ./zkCli.sh
Connecting to localhost:2181
log4j:WARN No appenders could be found for logger (org.apache.zookeeper.ZooKeeper).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Welcome to ZooKeeper!
JLine support is enabled

WATCHER::

WatchedEvent state:SyncConnected type:None path:null
[zk: localhost:2181(CONNECTING) 0] ls /
[zookeeper]
[zk: localhost:2181(CONNECTED) 1] get /zookeeper/config
server.1=zk1:2888:3888:participant;0.0.0.0:2181
server.2=zk2:2888:3888:participant;0.0.0.0:2181
server.3=zk3:2888:3888:participant;0.0.0.0:2181
version=0
[zk: localhost:2181(CONNECTED) 2] 
```

当启动一个 ZooKeeper 实例时，它会在预设的数据目录中（通常是 `dataDir` 参数指定的目录）创建一些预设的路径和节点。这些路径和节点包括：

1. **`/zookeeper`**: 这是 ZooKeeper 内部用于存储元数据的根节点。它包含了一些 ZooKeeper 服务器的状态信息。
2. **`/zookeeper/quota`**: 这个节点用于存储配额信息。ZooKeeper 支持配额管理，允许设置每个节点的最大限制。
3. **`/zookeeper/config`**: 包含 ZooKeeper 服务器的配置信息。
4. **`/zookeeper/available`**: 这个节点是用来通知客户端的，表示 ZooKeeper 服务器是可用的。

当启动一个全新的 ZooKeeper 实例时，它会在这些路径下创建这些节点，以确保 ZooKeeper 服务器正常运行并提供必要的服务。这些节点对于 ZooKeeper 服务器的正常运行是必需的，并且一般情况下不需要手动创建或干预这些节点。

---
