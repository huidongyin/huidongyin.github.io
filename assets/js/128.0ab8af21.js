(window.webpackJsonp=window.webpackJsonp||[]).push([[128],{470:function(e,t,o){"use strict";o.r(t);var v=o(4),_=Object(v.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("Kafka 将消息存储在磁盘中，为了控制磁盘占用空间的不断增加就需要对消息做一定的清理操作。Kafka 中每一个分区副本都对应一个 Log，而 Log 又可以分为多个日志分段，这样也便于日志的清理操作。Kafka 提供了两种日志清理策略。")]),e._v(" "),t("ol",[t("li",[e._v("日志删除(Log Retention):按照一定的保留策略直接删除不符合条件的日志分段。")]),e._v(" "),t("li",[e._v("日志压缩(Log Compaction):针对每个消息的 key 进行整合，对于有相同 key 的不同 value 值，只保留最后一个版本。")])]),e._v(" "),t("p",[e._v("我们可以通过 broker 端参数 "),t("code",[e._v("log.cleanup.policy")]),e._v(" 来设置日志清理策略，此参数的默认值为"),t("code",[e._v("delete")]),e._v("，即采用日志删除的清理策略。如果要采用日志压缩的清理策略，就需要将"),t("code",[e._v("log.cleanup.policy")]),e._v(" 设置为"),t("code",[e._v("compact")]),e._v("，并且还需要将 "),t("code",[e._v("log.cleaner.enable")]),e._v("(默认值为 true)设定为 true。通过将 "),t("code",[e._v("log.cleanup.policy")]),e._v(" 参数设置为"),t("code",[e._v("delete,compact")]),e._v("，还可以同时支持日志删除和日志压缩两种策略。日志清理的粒度可以控制到主题级别，比如与"),t("code",[e._v("log.cleanup.policy")]),e._v(" 对应的主题级别的参数为 "),t("code",[e._v("cleanup.policy")]),e._v("。")]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_1-日志删除"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-日志删除"}},[e._v("#")]),e._v(" 1.日志删除")]),e._v(" "),t("p",[e._v("在 Kafka 的日志管理器中会有一个专门的日志删除任务来周期性地检测和删除不符合保留条件的日志分段文件，这个周期可以通过 broker端参数 "),t("code",[e._v("log.retention.check.interval.ms")]),e._v("来配置，默认值为 300000，即 5 分钟。当前日志分段的保留策略有 3 种:基于时间的保留策略、基于日志大小的保留策略和基于日志起始偏移量的保留策略。")]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_1-1-基于时间"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-基于时间"}},[e._v("#")]),e._v(" 1.1 基于时间")]),e._v(" "),t("p",[e._v("日志删除任务会检查当前日志文件中是否有保留时间超过设定的阙值 ("),t("strong",[e._v("retentionMs")]),e._v(")来寻找可删除的日志分段文件集合("),t("strong",[e._v("deletableSegments")]),e._v("),如图所示。"),t("code",[e._v("retentionMs")]),e._v(" 可以通过 broker端参数 "),t("code",[e._v("log.retention.hours、log.retention.minutes 和log.retention.ms")]),e._v(" 来设置，其中 "),t("code",[e._v("log.retention.ms")]),e._v(" 的优先级最高，"),t("code",[e._v("log.retention.minutes")]),e._v(" 次之"),t("code",[e._v("log.retention.hours")]),e._v(" 最低。默认情况下只配置了 "),t("code",[e._v("log.retention.hours")]),e._v(" 参数，其值为168，故默认情况下日志分段文件的保留时间为 7 天。")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311202121914.png",alt:""}})]),e._v(" "),t("p",[e._v("查找过期的日志分段文件，并不是简单地根据日志分段的最近修改时间 "),t("code",[e._v("lastModifiedTime")]),e._v(" 来计算的，而是根据日志分段中最大的时间戳 "),t("code",[e._v("largestTimeStamp")]),e._v(" 来计算的。因为日志分段的 "),t("code",[e._v("lastModifiedTime")]),e._v(" 可以被有意或无意地修改，比如执行了 "),t("code",[e._v("touch")]),e._v(" 操作，或者分区副本进行了重新分配，"),t("code",[e._v("lastModifiedTime")]),e._v(" 并不能真实地反映出日志分段在磁盘的保留时间。要获取日志分段中最大时间戳 "),t("code",[e._v("largestTimeStamp")]),e._v(" 的值，首先要查询该日志分段所对应的时间戳索引文件，查找时间戳索引文件中最后一条索引项，若最后一条索引项的时间戳字段值大于 0，则取其值，否则 才设置为最近修改时间 "),t("code",[e._v("lastModifiedTime")]),e._v("。")]),e._v(" "),t("p",[e._v("若待删除的日志分段的总数等于该日志文件中所有的日志分段的数量，那么说明所有的日志分段都已过期，但该日志文件中还要有一个日志分段用于接收消息的写入，即必须要保证有一个活跃的日志分段 "),t("code",[e._v("activeSegment")]),e._v("，在此种情况下，会先切分出一个新的日志分段作为"),t("code",[e._v("activeSegment")]),e._v("，然后执行删除操作。")]),e._v(" "),t("p",[e._v("删除日志分段时,首先会从 Log 对象中所维护日志分段的跳跃表中移除待删除的日志分段以保证没有线程对这些日志分段进行读取操作。然后将日志分段所对应的所有文件添加上"),t("code",[e._v(".deleted")]),e._v("的后缀(当然也包括对应的索引文件)。最后交由一个以"),t("code",[e._v("delete-file")]),e._v("命名的延迟任务来删除这些以"),t("code",[e._v(".deleted")]),e._v("为后缀的文件，这个任务的延迟执行时间可以通过 "),t("code",[e._v("file.delete.delay.ms")]),e._v(" 参数来调配，此参数的默认值为 60000，即 1分钟。")]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_1-2-基于日志大小"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-基于日志大小"}},[e._v("#")]),e._v(" 1.2 基于日志大小")]),e._v(" "),t("p",[e._v("日志删除任务会检查当前日志的大小是否超过设定的阙值 ("),t("strong",[e._v("retentionSize")]),e._v(")来寻找可删除的日志分段的文件集合("),t("strong",[e._v("deletableSegments")]),e._v(")，如图所示。"),t("code",[e._v("retentionSize")]),e._v(" 可以通过 broker 端参数 "),t("code",[e._v("log.retention.bytes")]),e._v(" 来配置,默认值为-1,表示无穷大。注意 "),t("code",[e._v("log.retention.bytes")]),e._v(" 配置的是 Log 中所有日志文件的总大小，而不是单个日志分段(确切地说应该为"),t("code",[e._v(".log")]),e._v("日志文件)的大小。单个日志分段的大小由 broker 端参数 "),t("code",[e._v("log.segment.bytes")]),e._v(" 来限制，默认值为1073741824，即 1GB。")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311202127338.png",alt:""}})]),e._v(" "),t("p",[e._v("基于日志大小的保留策略与基于时间的保留策略类似，首先计算日志文件的总大小 "),t("code",[e._v("size 和retentionSize")]),e._v(" 的差值 "),t("code",[e._v("diff")]),e._v("，即计算需要删除的日志总大小，然后从日志文件中的第一个日志分段开始进行查找可删除的日志分段的文件集合 "),t("code",[e._v("deletableSegments")]),e._v("。查找出 "),t("code",[e._v("deletableSegments")]),e._v(" 之后就执行删除操作，这个删除操作和基于时间的保留策略的删除操作相同。")]),e._v(" "),t("hr"),e._v(" "),t("h3",{attrs:{id:"_1-3-基于日志起始偏移量"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-基于日志起始偏移量"}},[e._v("#")]),e._v(" 1.3 基于日志起始偏移量")]),e._v(" "),t("p",[e._v("一般情况下，日志文件的起始偏移量 "),t("code",[e._v("logStartOffset")]),e._v(" 等于第一个日志分段的 "),t("code",[e._v("baseOffset")]),e._v("，但这并不是绝对的，"),t("code",[e._v("logStartOffset")]),e._v(" 的值可以通过 "),t("code",[e._v("DeleteRecordsRequest")]),e._v(" 请求 (比如使用"),t("code",[e._v("KafkaAdminClient")]),e._v(" 的 "),t("code",[e._v("deleteRecords()")]),e._v("方法、使用 "),t("code",[e._v("kafka-delete-records.sh")]),e._v(" 脚本)、日志的清理和截断等操作进行修改。")]),e._v(" "),t("p",[e._v("基于日志起始偏移量的保留策略的判断依据是某日志分段的下一个日志分段的起始偏移量"),t("code",[e._v("baseOffset")]),e._v(" 是否小于等于 "),t("code",[e._v("logStartOffset")]),e._v("，若是，则可以删除此日志分段。如图所示，假设"),t("code",[e._v("logStartOffset")]),e._v(" 等于 25，日志分段 1的起始偏移量为 0，日志分段 2 的起始偏移量为 11，日志分段3 的起始偏移量为 23，通过如下动作收集可删除的日志分段的文件集合 "),t("code",[e._v("deletableSegments")]),e._v(":")]),e._v(" "),t("ol",[t("li",[e._v("从头开始遍历每个日志分段，日志分段 1 的下一个日志分段的起始偏移量为 11，小于"),t("code",[e._v("logStartOffset")]),e._v(" 的大小，将日志分段1加入 "),t("code",[e._v("deletableSegments")]),e._v("。")]),e._v(" "),t("li",[e._v("日志分段 2 的下一个日志偏移量的起始偏移量为 23，也小于 "),t("code",[e._v("logStartOffset")]),e._v(" 的大小,将日志分段 2也加入 "),t("code",[e._v("deletableSegments")]),e._v("。")]),e._v(" "),t("li",[e._v("日志分段 3 的下一个日志偏移量在"),t("code",[e._v("logStartOffset")]),e._v("的右侧，故从日志分段 3 开始的所有日志分段都不会加入 "),t("code",[e._v("deletableSegments")]),e._v("。")])]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311202131435.png",alt:""}})]),e._v(" "),t("p",[e._v("收集完可以删除的日志分段的文件集合之后的删除操作同基于日志大小的保留策略和基于时间的保留策略相同。")]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_2-日志压缩"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-日志压缩"}},[e._v("#")]),e._v(" 2.日志压缩")]),e._v(" "),t("p",[e._v("Kafka 中的 Log Compaction 是指在默认的日志删除(Log Retention) 规则之外提供的一种清理过时数据的方式。如图所示，Log Compaction 对于有相同 key 的不同 value 值，只保留最后一个版本。如果应用只关心 key 对应的最新 value 值,则可以开启 Kafka 的日志清理功能,Kafka 会定期将相同 key 的消息进行合并，只保留最新的 value 值。")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311202154862.png",alt:""}})]),e._v(" "),t("p",[e._v("Log Compaction 执行前后，日志分段中的每条消息的偏移量和写入时的偏移量保持一致。Log Compaction 会生成新的日志分段文件，日志分段中每条消息的物理位置会重新按照新文件来组织。Log Compaction 执行过后的偏移量不再是连续的，不过这并不影响日志的查询。")]),e._v(" "),t("p",[e._v("Kafka 中的 Log Compaction 可以类比于 Redis 中的 RDB 的持久化模式。试想一下，如果一个系统使用 Kafka 来保存状态，那么每次有状态变更都会将其写入 Kafka。在某一时刻此系统异常崩溃，进而在恢复时通过读取 Kafka 中的消息来恢复其应有的状态，那么此系统关心的是它原本的最新状态而不是历史时刻中的每一个状态。如果 Kafka 的日志保存策略是日志删除(Log Deletion)，那么系统势必要一股脑地读取 Kafka 中的所有数据来进行恢复，如果日志保存策略是 Log Compaction，那么可以减少数据的加载量进而加快系统的恢复速度。LogCompaction 在某些应用场景下可以简化技术栈，提高系统整体的质量。")]),e._v(" "),t("p",[e._v("我们知道可以通过配置 "),t("code",[e._v("log.dir 或 log.dirs")]),e._v(" 参数来设置 Kafka 日志的存放目录，而每个日志目录下都有一个名为"),t("code",[e._v("cleaner-offset-checkpoint")]),e._v("的文件，这个文件就是清理检查点文件，用来记录每个主题的每个分区中已清理的偏移量。通过清理检查点文件可以将 Log 分成两个部分，如图所示。通过检查点 "),t("code",[e._v("cleaner checkpoint")]),e._v(" 来划分出一个已经清理过的 "),t("strong",[e._v("clean")]),e._v(" 部分和一个还未清理过的 "),t("strong",[e._v("dirty")]),e._v(" 部分。在日志清理的同时，客户端也可以读取日志中的消息。"),t("strong",[e._v("dirty")]),e._v(" 部分的消息偏移量是逐一递增的，而 "),t("strong",[e._v("clean")]),e._v(" 部分的消息偏移量是断续的，如果客户端总能赶上 "),t("strong",[e._v("dirty")]),e._v(" 部分，那么它就能读取日志的所有消息，反之就不可能读到全部的消息。")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311202157265.png",alt:""}})]),e._v(" "),t("p",[e._v("上图中的 "),t("code",[e._v("firstDirtyOffset")]),e._v(" (与 "),t("code",[e._v("cleaner checkpoint")]),e._v(" 相等)表示 "),t("strong",[e._v("dirty")]),e._v(" 部分的起始偏移量，而"),t("code",[e._v("firstUncleanableOffset")]),e._v("为 "),t("strong",[e._v("dirty")]),e._v(" 部分的截止偏移量，整个 "),t("strong",[e._v("dirty")]),e._v(" 部分的偏移量范围为"),t("code",[e._v("[firstDirtyOffset,firstUncleanableOffset)")]),e._v("，这里是左闭右开区间。为了避免当前活跃的日志分段 "),t("code",[e._v("activeSegment")]),e._v("成为热点文件，"),t("code",[e._v("activeSegment")]),e._v(" 不会参与 Log Compaction 的执行。同时 Kafka 支持通过参数"),t("code",[e._v("log.cleaner.min.compaction.lag.ms")]),e._v("(默认值为 0)来配置消息在被清理前的最小保留时间，默认情况下 "),t("code",[e._v("firstUncleanableOffset")]),e._v(" 等于 "),t("code",[e._v("activeSegment")]),e._v(" 的 "),t("code",[e._v("baseOffset")]),e._v("。")]),e._v(" "),t("p",[e._v("Log Compaction 是针对 key 的，所以在使用时应注意每个消息的 key 值不为 null。每个 broker 会启动 "),t("code",[e._v("log.cleaner.thread")]),e._v(" (默认值为 1)个日志清理线程负责执行清理任务,这些线程会选择"),t("strong",[e._v("污浊率")]),e._v("最高的日志文件进行清理。用 "),t("code",[e._v("cleanBytes")]),e._v(" 表示 "),t("strong",[e._v("clean")]),e._v(" 部分的日志占用大小，"),t("code",[e._v("dirtyBytes")]),e._v(" 表示 "),t("strong",[e._v("dirty")]),e._v(" 部分的日志占用大小，那么这个日志的污浊率("),t("strong",[e._v("dirtyRatio")]),e._v(")为:")]),e._v(" "),t("div",{staticClass:"language-text line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("dirtyRatio = dirtyBytes / (cleanBytes + dirtyBytes)\n")])]),e._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[e._v("1")]),t("br")])]),t("p",[e._v("为了防止日志不必要的频繁清理操作，Kafka 还使用了参数 "),t("code",[e._v("log.cleaner.min.cleanable.ratio")]),e._v("(默认值为 0.5)来限定可进行清理操作的最小污浊率。Kafka 中用于保存消费者消费位移的主题"),t("code",[e._v("_consumer_offsets")]),e._v(" 使用的就是 Log Compaction 策略。")]),e._v(" "),t("p",[e._v("我们已经知道怎样选择合适的日志文件做清理操作,然而怎么对日志文件中消息的 key 进行筛选操作呢? Kafka 中的每个日志清理线程会使用一个名为"),t("strong",[e._v("SkimpyOffsetMap")]),e._v("的对象来构建 key 与 offset 的映射关系的哈希表。日志清理需要遍历两次日志文件，第一次遍历把每个key 的哈希值和最后出现的 offset 都保存在 "),t("strong",[e._v("SkimpyOffsetMap")]),e._v(" 中，映射模型如图所示。第二次遍历会检查每个消息是否符合保留条件，如果符合就保留下来，否则就会被清理。假设一条消息的 offset 为 O1，这条消息的 key 在 "),t("strong",[e._v("SkimpyOffsetMap")]),e._v(" 中对应的 offset 为 O2，如果 O1 大于等于 O2 即满足保留条件。")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311202204120.png",alt:""}})]),e._v(" "),t("p",[e._v("默认情况下，"),t("strong",[e._v("SkimpyOffsetMap")]),e._v(" 使用 MD5 来计算 key 的哈希值，占用空间大小为 16B，根据这个哈希值来从 "),t("strong",[e._v("SkimpyOffsetMap")]),e._v(" 中找到对应的槽位，如果发生冲突则用线性探测法处理。为了防止哈希冲突过于频繁，也可以通过 broker 端参数 "),t("code",[e._v("log.cleaner.io.buffer.load.factor")]),e._v("(默认值为 0.9)来调整负载因子。偏移量占用空间大小为8B，故一个映射项占用大小为24B。每个日志清理线程的 "),t("strong",[e._v("SkimpyOffsetMap")]),e._v(" 的内存占用大小为"),t("code",[e._v("log.cleaner.dedupe.buffer.size/log.cleaner.thread")]),e._v("，默认值为 "),t("code",[e._v("128MB/1=128MB")]),e._v("。所以默认情况下 "),t("strong",[e._v("SkimpyOffsetMap")]),e._v(" 可以保存"),t("code",[e._v("128MB X 0.9 /24B ≈ 5033164")]),e._v(" 个 key 的记录。假设每条消息的大小为 1KB，"),t("strong",[e._v("那么这个SkimpyOffsetMap")]),e._v(" 可以用来映射 4.8GB 的日志文件,如果有重复的 key,那么这个数值还会增大,整体上来说，"),t("strong",[e._v("SkimpyOffsetMap")]),e._v(" 极大地节省了内存空间且非常高效。")]),e._v(" "),t("p",[e._v("Log Compaction 会保留 key 相应的最新value 值,那么当需要删除一个 key时怎么办?Kafka提供了一个墓碑消息 (tombstone)的概念，如果一条消息的 key 不为 null,但是其 value 为 null,那么此消息就是墓碑消息。日志清理线程发现墓碑消息时会先进行常规的清理，并保留墓碑消息一段时间。墓碑消息的保留条件是当前墓碑消息所在的日志分段的最近修改时间 "),t("code",[e._v("lastModifiedTime")]),e._v(" 大于 "),t("code",[e._v("deleteHorizonMs")]),e._v("。这个"),t("code",[e._v("deleteHorizonMs")]),e._v("的计算方式为clean部分中最后一个日志分段的最近修改时间减去保留值 "),t("code",[e._v("deleteRentionMs")]),e._v(" (通过 broker 端参数"),t("code",[e._v("log.cleaner.delete.retention.ms")]),e._v(" 配置,默认值为 86400000,即24小时)的大小,即:")]),e._v(" "),t("div",{staticClass:"language-text line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("deleteHorizonMs=clean 部分中最后一个 LogSegment 的lastModifiedTime - deleteRetionMs\n")])]),e._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[e._v("1")]),t("br")])]),t("p",[e._v("所以墓碑消息的保留条件为:")]),e._v(" "),t("div",{staticClass:"language-text line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("所在 LogSegment 的 lastModifiedTime > deleteHorizonMs\n=> 所在 LogSegment 的 lastModifiedTime > clean 部分中最后一个 LogSegment 的lastModifiedTime - deleteRetionMs\n=> 所在 LogSegment 的 lastModifiedTime + deleteRetionMs > clean 部分中最后一个LogSegment 的 lastModifiedTime\n")])]),e._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[e._v("1")]),t("br"),t("span",{staticClass:"line-number"},[e._v("2")]),t("br"),t("span",{staticClass:"line-number"},[e._v("3")]),t("br")])]),t("p",[e._v("Log Compaction 执行过后的日志分段的大小会比原先的日志分段的要小，为了防止出现太多的小文件，Kafka 在实际清理过程中并不对单个的日志分段进行单独清理，而是将日志文件中 offset 从 0至 "),t("code",[e._v("firstUncleanableOffset")]),e._v(" 的所有日志分段进行分组，每个日志分段只属于一组，分组策略为:按照日志分段的顺序遍历，每组中日志分段的占用空间大小之和不超过"),t("code",[e._v("segmentSize")]),e._v("(可以通过 broker 端参数 "),t("code",[e._v("log.segment.bytes")]),e._v(" 设置，默认值为1GB)，且对应的索引文件占用大小之和不超过 "),t("code",[e._v("maxIndexSize")]),e._v(" (可以通过 broker 端参数 "),t("code",[e._v("log.index.size.max.bytes")]),e._v("设置，默认值为 10MB)。同一个组的多个日志分段清理过后，只会生成一个新的日志分段。")]),e._v(" "),t("p",[e._v("如图所示，假设所有的参数配置都为默认值，在 Log Compaction 之前 "),t("code",[e._v("checkpoint")]),e._v(" 的初始值为 0。执行第一次 "),t("strong",[e._v("Log Compaction")]),e._v(" 之后，每个非活跃的日志分段的大小都有所缩减,"),t("code",[e._v("checkpoint")]),e._v(" 的值也有所变化。执行第二次 "),t("strong",[e._v("Log Compaction")]),e._v(" 时会组队成"),t("code",[e._v("[0.4GB0.4GB]")]),e._v("、"),t("code",[e._v("[0.3GB0.7GB]")]),e._v("、"),t("code",[e._v("[0.3GB]")]),e._v("、"),t("code",[e._v("[1GB]")]),e._v("这4 个分组，并且从第二次 "),t("strong",[e._v("Log Compaction")]),e._v(" 开始还会涉及墓碑消息清除。同理，第三次 "),t("strong",[e._v("Log Compaction")]),e._v(" 过后的情形可参考下图的尾部。"),t("strong",[e._v("Log Compaction")]),e._v(" 过程中会将每个日志分组中需要保留的消息复制到一个以"),t("code",[e._v(".clean")]),e._v("为后缀的临时文件中，此临时文件以当前日志分组中第一个日志分段的文件名命名，例如 "),t("code",[e._v("00000000000000000000.log.clean")]),e._v("。"),t("strong",[e._v("LogCompaction")]),e._v(" 过后将"),t("code",[e._v(".clean")]),e._v("的文件修改为"),t("code",[e._v(".swap")]),e._v("后缀的文件，例如:"),t("code",[e._v("00000000000000000000.log.swap")]),e._v("。然后删除原本的日志文件，最后才把文件的"),t("code",[e._v(".swap")]),e._v("后缀去掉。整个过程中的索引文件的变换也是如此，至此一个完整 "),t("strong",[e._v("Log Compaction")]),e._v(" 操作才算完成。")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311202219583.png",alt:""}})]),e._v(" "),t("blockquote",[t("p",[e._v("注意日志压缩和日志清除的区别：日志清除是指清除整个日志分段，而日志压缩是针对相同key的消息的合并清理。")])]),e._v(" "),t("hr")])}),[],!1,null,null,null);t.default=_.exports}}]);