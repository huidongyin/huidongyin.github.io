(window.webpackJsonp=window.webpackJsonp||[]).push([[127],{469:function(e,t,s){"use strict";s.r(t);var v=s(4),a=Object(v.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("每个日志分段文件对应了两个索引文件，主要用来提高查找消息的效率。偏移量索引文件用来建立消息偏移量到物理地址之间的映射关系，方便快速定位消息所在的物理文件的位置；时间戳索引文件则根据指定的时间戳来查找对应的偏移量信息。")]),e._v(" "),t("p",[e._v("Kafka中的索引文件以稀疏索引的方式构造消息的索引，他并不保证每一个消息在索引文件中都有对应的索引项。每当写入一定量的消息时（一定量由broker参数"),t("code",[e._v("log.index.interval.bytes")]),e._v("指定，默认4096，,4KB），偏移量索引文件和时间戳索引文件分别增加一个偏移量索引项和时间戳索引项。")]),e._v(" "),t("p",[e._v("稀疏索引通过 MappedByteBuffer 将索引文件映射到内存中;以加快索引的查询速度。偏移量索引文件中的偏移量是单调递增的，查询指定偏移量时使用二分法来快速定位偏移量的位置，如果指定的偏移量不在索引文件中，则会返回小于指定偏移量的最大偏移量。时间戳索引文件中的时间戳也保持严格的单调递增，查询指定时间戳时，也根据二分查找法来查找不大于该时间戳的最大偏移量，至于要找到对应的物理文件位置还需要根据偏移量索引文件来进行再次定位。稀疏索引的方式是在磁盘空间、内存空间、查找时间等多方面之间的一个折中。")]),e._v(" "),t("p",[e._v("日志分段文件达到一定条件时需要进行切分，那么其对应的索引文件也需要进行切分。日志分段文件切分包含以下几个条件，满足其一即可。")]),e._v(" "),t("ol",[t("li",[e._v("当前日志分段文件大小超过了broker端参数"),t("code",[e._v("log.segment.bytes")]),e._v("配置的值，默认是1GB。")]),e._v(" "),t("li",[e._v("当前日志分段中消息的最大时间戳于当前系统的时间戳的差值大于"),t("code",[e._v("log.roll.ms")]),e._v("或"),t("code",[e._v("log.roll.hours")]),e._v("参数配置的值。如果同时配置了"),t("code",[e._v("log.roll.ms")]),e._v("和"),t("code",[e._v("log.roll.hours")]),e._v("参数，"),t("code",[e._v("log.roll.ms")]),e._v("的优先级更高。默认仅仅配置了"),t("code",[e._v("log.roll.hours")]),e._v("，值为168，即7天。")]),e._v(" "),t("li",[e._v("偏移量索引文件或时间戳索引文件的大小达到了broker端参数"),t("code",[e._v("log.index.size.max.bytes")]),e._v("配置的值。默认10MB。")]),e._v(" "),t("li",[e._v("追加的消息的偏移量与当前日志分段的偏移量之间的差值大于"),t("code",[e._v("Integer.MAX_VALUE")]),e._v("，即要追加的消息的偏移量不能转变为相对偏移量。")])]),e._v(" "),t("p",[e._v("对非活跃的日志分段而言，其对应的索引文件的内容已经固定而不需要再写入索引项，所以会被设定为只读。而对当前活跃的日志分段而言，索引文件还会追加更多的索引项，所以被设定为可读写。在索引文件切分的时候，Kafka 会关闭当前正在写入的索引文件并置为只读模式，同时以可读写的模式创建新的索引文件，索引文件的大小由 broker 端参数 "),t("code",[e._v("log.index.size.max.bytes")]),e._v(" 配置。Kafka 在创建索引文件的时候会为其预分配"),t("code",[e._v("log.index.size.max.bytes")]),e._v(" 大小的空间，注意这一点与日志分段文件不同，只有当索引文件进行切分的时候，Kafka 才会把该索引文件裁剪到实际的数据大小。也就是说，与当前活跃的日志分段对应的索引文件的大小固定为 "),t("code",[e._v("log.index.size.max.bytes")]),e._v("，而其余日志分段对应的索引文件的大小为实际的占用空间。")]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_1-偏移量索引"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-偏移量索引"}},[e._v("#")]),e._v(" 1.偏移量索引")]),e._v(" "),t("p",[e._v("偏移量索引项的格式如图所示。每个索引项占用 8 个字节，分为两个部分。")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311202111390.png",alt:""}})]),e._v(" "),t("ol",[t("li",[t("code",[e._v("relativeOffset")]),e._v(": 相对偏移量，表示消息相对于 baseOffset 的偏移量，占用 4 个字节,当前索引文件的文件名即为 baseOffset 的值。")]),e._v(" "),t("li",[t("code",[e._v("position")]),e._v(":物理地址，也就是消息在日志分段文件中对应的物理位置，占用 4 个字节。")])]),e._v(" "),t("p",[e._v("消息的偏移量 (offset)占用 8 个字节，也可以称为绝对偏移量。索引项中没有直接使用绝对偏移量而改为只占用 4 个字节的相对偏移量 (relativeOffset= offset - baseOffset)，这样可以减小索引文件占用的空间。举个例子，一个日志分段的 baseOffset 为 32，那么其文件名就是00000000000000000032.log，offset为35 的消息在索引文件中的 relativeOffset 的值为35-32=3。")]),e._v(" "),t("p",[t("strong",[e._v("前面日志分段文件切分的第 4 个条件:追加的消息的偏移量与当前日志分段的偏移量之间的差值大于 IntegerMAX_VALUE")]),e._v("。如果彼此的差值超过了 "),t("strong",[e._v("Integer.MAX_VALUE")]),e._v("那么 relativeOffset 就不能用 4 个字节表示了，进而不能享受这个索引项的设计所带来的便利了。")]),e._v(" "),t("p",[e._v("可以使用前面讲的 "),t("code",[e._v("kafka-dump-log.sh")]),e._v(" 脚本来解析"),t("code",[e._v(".index")]),e._v(" 文件(还包括"),t("code",[e._v(".timeindex、.snapshot、.txnindex")]),e._v(" 等文件)，示例如下:")]),e._v(" "),t("div",{staticClass:"language-text line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("./kafka-dump-log.sh --files /tmp/kafka-logs/topic-log-0/00000000000000000000.index\n")])]),e._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[e._v("1")]),t("br")])]),t("p",[e._v("结果如下：")]),e._v(" "),t("div",{staticClass:"language-text line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("offset: 6 position: 156\noffset: 14 position: 459\noffset: 22 position: 656\noffset: 26 position: 838\noffset: 31 position: 1050\n")])]),e._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[e._v("1")]),t("br"),t("span",{staticClass:"line-number"},[e._v("2")]),t("br"),t("span",{staticClass:"line-number"},[e._v("3")]),t("br"),t("span",{staticClass:"line-number"},[e._v("4")]),t("br"),t("span",{staticClass:"line-number"},[e._v("5")]),t("br")])]),t("p",[e._v("如果我们要查找偏移量为 23 的消息，那么应该怎么做呢？首先通过二分法在偏移量索引文件中找到不大于 23 的最大索引项，即"),t("code",[e._v("[22,656]")]),e._v("，然后从日志分段文件中的物理位置 656 开始顺序查找偏移量为 23 的消息。")]),e._v(" "),t("p",[e._v("以上是最简单的一种情况。参考下图，如果要查找偏移量为 268 的消息，那么应该怎么办呢?首先肯定是定位到baseOffset为251的日志分段,然后计算相对偏移量relativeOffset=268-251=17，之后再在对应的索引文件中找到不大于 17 的索引项，最后根据索引项中的 position定位到具体的日志分段文件位置开始查找目标消息。那么又是如何查找 baseOffset 为 251 的日志分段的呢?这里并不是顺序查找，而是用了跳跃表的结构。Kafka 的每个日志对象中使用了"),t("strong",[e._v("ConcurrentSkipListMap")]),e._v(" 来保存各个日志分段，每个日志分段的 baseOffset 作为 key，这样可以根据指定偏移量来快速定位到消息所在的日志分段。")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311202112041.png",alt:""}})]),e._v(" "),t("p",[e._v("Kafka 强制要求索引文件大小必须是索引项大小的整数倍，对偏移量索引文件而言，必须为8 的整数倍。如果 broker 端参数 "),t("code",[e._v("log.index.size.max.bytes")]),e._v(" 配置为67，那么 Kafka 在内部会将其转换为 64，即不大于 67，并且满足为 8 的整数倍的条件。")]),e._v(" "),t("hr"),e._v(" "),t("h2",{attrs:{id:"_2-时间戳索引"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-时间戳索引"}},[e._v("#")]),e._v(" 2.时间戳索引")]),e._v(" "),t("p",[e._v("时间戳索引项的格式如图。")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://raw.githubusercontent.com/huidongyin/DrawingBed/main/kafka/202311202112405.png",alt:""}})]),e._v(" "),t("p",[e._v("每个索引项占用 12 个字节，分为两个部分。")]),e._v(" "),t("ol",[t("li",[e._v("timestamp:当前日志分段最大的时间戳。")]),e._v(" "),t("li",[e._v("relativeOffset:时间戳所对应的消息的相对偏移量。")])]),e._v(" "),t("p",[e._v("时间戳索引文件中包含若干时间戳索引项，每个追加的时间戳索引项中的 timestamp 必须大于之前追加的索引项的 timestamp，否则不予追加。如果 broker 端参数 "),t("code",[e._v("log.message.timestamp.type")]),e._v(" 设置为 "),t("strong",[e._v("LogAppendTime")]),e._v("，那么消息的时间戳必定能够保持单调递增;相反如果是 "),t("strong",[e._v("CreateTime")]),e._v(" 类型则无法保证。生产者可以使用类似 "),t("code",[e._v("ProducerRecord(String topic,Integer partition,Long timestamp,K key,V value)")]),e._v("的方法来指定时间戳的值。即使生产者客户端采用自动插入的时间藏也无法保证时间截能够单调递增，如果两个不同时钟的生产者同时往一个分区中插入消息，那么也会造成当前分区的时间戳乱序。")]),e._v(" "),t("p",[e._v("与偏移量素引文件相似，时间戳索引文件大小必须是索引项大小 (12B) 的整数倍，如果不满足条件也会进行裁剪。同样假设 broker端参数 "),t("code",[e._v("log.index.size.max.bytes")]),e._v(" 配置为 67,那么对应于时间戳索引文件，Kafka 在内部会将其转换为 60。")]),e._v(" "),t("p",[e._v("我们已经知道每当写入一定量的消息时，就会在偏移量索引文件和时间戳索引文件中分别增加一个偏移量索引项和时间戳索引项。两个文件增加索引项的操作是同时进行的，但并不意味着偏移量索引中的 "),t("code",[e._v("relativeOffset")]),e._v(" 和时间戳索引项中的 "),t("code",[e._v("relativeOffset")]),e._v(" 是同一个值。")]),e._v(" "),t("p",[e._v("如果要查找指定时间戳 targetTimeStamp =1526384718288 开始的消息，首先是找到不小于指定时间戳的日志分段。这里就无法使用跳跃表来快速定位到相应的日志分段了，需要分以下几个步骤来完成。")]),e._v(" "),t("ol",[t("li",[e._v("将 targetTimeStamp和每个日志分段中的最大时间戳 largestTimeStamp 逐一对比，直到找到不小于 targetTimeStamp 的 largestTimeStamp 所对应的日志分段。日志分段中的largestTimeStamp 的计算是先查询该日志分段所对应的时间戳索引文件，找到最后一条索引项,若最后一条索引项的时间戳字段值大于 0，则取其值，否则取该日志分段的最近修改时间。")]),e._v(" "),t("li",[e._v("找到相应的日志分段之后，在时间戳索引文件中使用二分查找算法查找到不大于targetTimeStamp 的最大索引项，即"),t("code",[e._v("[1526384718283,28]")]),e._v("，如此便找到了一个相对偏移量 28。")]),e._v(" "),t("li",[e._v("在偏移量索引文件中使用二分算法查找到不大于 28 的最大索引项，即"),t("code",[e._v("[26,838]")]),e._v("。")]),e._v(" "),t("li",[e._v("从步骤1中找到日志分段文件中的 838 的物理位置开始查找不小于 targetTimeStamp 的消息。")])]),e._v(" "),t("hr")])}),[],!1,null,null,null);t.default=a.exports}}]);